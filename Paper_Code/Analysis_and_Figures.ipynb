{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures and Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import math\n",
    "\n",
    "def add_subplot_tags_and_stack(directory, prefix, num_columns=1, give_list_instead=None):\n",
    "    # List of images that start with the given prefix\n",
    "    if give_list_instead:\n",
    "        images=give_list_instead\n",
    "    else: \n",
    "        images = [os.path.join(directory, f) for f in os.listdir(directory) if f.startswith(prefix) and f.endswith('.png')]\n",
    "        # Sort images if necessary\n",
    "        images.sort()\n",
    "    \n",
    "    # Load images and prepare for tagging\n",
    "    loaded_images = [Image.open(img_path) for img_path in images]\n",
    "    \n",
    "    # Prepare the subplot tags (a, b, c, ...)\n",
    "    subplot_tags = [chr(97 + i) for i in range(len(loaded_images))]  # 'a' is 97 in ASCII\n",
    "    \n",
    "    # List to hold modified images\n",
    "    tagged_images = []\n",
    "    \n",
    "    # Font for subplot label\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 16)  # You may need to adjust the path to arial.ttf or use another font\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    # Process each image\n",
    "    for image, tag in zip(loaded_images, subplot_tags):\n",
    "        # Draw the subplot tag on the image\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        text = f\"({tag})\"\n",
    "        box_size = 25  # Define the size of the box for the subplot label\n",
    "        draw.rectangle([(0, 0), (box_size, box_size)], outline=\"black\", fill=\"white\")\n",
    "        draw.text((25, 25), text, font=font, fill=\"black\")\n",
    "        tagged_images.append(image)\n",
    "    \n",
    "    # Determine layout\n",
    "    rows = math.ceil(len(tagged_images) / num_columns)\n",
    "    max_width = max(img.width for img in tagged_images)\n",
    "    max_height = max(img.height for img in tagged_images)\n",
    "\n",
    "    # Create a new image to stack all tagged images\n",
    "    total_width = num_columns * max_width\n",
    "    total_height = rows * max_height\n",
    "    stacked_image = Image.new('RGB', (total_width, total_height), 'white')\n",
    "    \n",
    "    # Paste images into the stacked_image\n",
    "    x_offset = 0\n",
    "    y_offset = 0\n",
    "\n",
    "    for i, img in enumerate(tagged_images):\n",
    "        stacked_image.paste(img, (x_offset, y_offset))\n",
    "        x_offset += img.width\n",
    "        if (i + 1) % num_columns == 0:\n",
    "            x_offset = 0\n",
    "            y_offset += img.height\n",
    "    \n",
    "    # Save the stacked image\n",
    "    stacked_image.save(os.path.join(\"C:\\\\Users\\\\LEGION\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\Submit\\\\Final Figures\", f\"{prefix}__stacked_image.png\"), bbox_inches='tight')\n",
    "    stacked_image.save(os.path.join(\"C:\\\\Users\\\\LEGION\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\Submit\\\\Final Figures\", f\"{prefix}__stacked_image.pdf\"), bbox_inches='tight')\n",
    "    stacked_image.show()\n",
    "    \n",
    "# Example usage\n",
    "#directory = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figures\"\n",
    "#prefix = 'E1_Performance_subcategory-'\n",
    "#num_columns=2\n",
    "#add_subplot_tags_and_stack(directory, prefix,num_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### E0: Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplemantary Table S4 \n",
    "E0P0_prompteng_functionvsopen_list=[r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\E0P0-opencall.xlsx\",\n",
    "                                    r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\E0P0-functioncall.xlsx\",\n",
    "                                    r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\E0P0-Langchain.xlsx\",\n",
    "                                    ]\n",
    "#analyze_model_accuracy(prompteng_functioncall_raw)\n",
    "analyze_and_merge_multiple_files(E0P0_prompteng_functionvsopen_list).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "functions = ['OpenAI Open Call', 'OpenAI Function Call', 'Lang Chain']\n",
    "gpt3_api_accuracy = [40.00, 43.33, 40.00]\n",
    "gpt4_api_accuracy = [55.00, 68.33, 66.67]\n",
    "\n",
    "bar_width = 0.4  # Width of the bars\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot grid\n",
    "plt.grid(True, linestyle='--', zorder=0)\n",
    "\n",
    "# Plot data bars\n",
    "plt.barh(range(len(functions)), gpt3_api_accuracy, height=bar_width, color='blue', label='GPT3-API', zorder=2)\n",
    "plt.barh([x + bar_width for x in range(len(functions))], gpt4_api_accuracy, height=bar_width, color='red', label='GPT4-API', zorder=2)\n",
    "\n",
    "plt.xlabel('Accuracy (%)', fontsize=14)\n",
    "plt.ylabel('Function', fontsize=14)\n",
    "plt.title('Accuracy of Different Functions', fontsize=16)\n",
    "plt.yticks([x + bar_width / 2 for x in range(len(functions))], functions, fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### E0: Prompt Eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from scipy.stats import wilcoxon\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def bootstrap_mean_diff_ci(data, reference, num_bootstrap=10000, alpha=0.05):\n",
    "    boot_diffs = []\n",
    "    n = len(data)\n",
    "    for _ in range(num_bootstrap):\n",
    "        sample_data = resample(data, n_samples=n, random_state=None)\n",
    "        sample_reference = resample(reference, n_samples=n, random_state=None)\n",
    "        boot_diffs.append(np.mean(sample_data) - np.mean(sample_reference))\n",
    "    lower_bound = np.percentile(boot_diffs, (alpha/2)*100)\n",
    "    upper_bound = np.percentile(boot_diffs, (1 - alpha/2)*100)\n",
    "    return np.round((lower_bound, upper_bound), 2)\n",
    "\n",
    "\n",
    "# def average_performance_columns_with_stats(df_acc, experimentname=''):\n",
    "#     df = df_acc.copy()\n",
    "#     # Regular expression to extract accuracy and correct answers\n",
    "#     accuracy_pattern = re.compile(r'(\\d+\\.?\\d*)% \\((\\d+)-of-(\\d+); Error: (\\d+)\\)')\n",
    "    \n",
    "#     # Determine performance columns by excluding 'Model Name'\n",
    "#     performance_columns = [col for col in df.columns if col != 'Model Name']\n",
    "    \n",
    "#     # Initialize new columns for averages, standard deviations, and confidence intervals\n",
    "\n",
    "#     raw_percents = []\n",
    "#     # Iterate over the DataFrame to calculate statistics\n",
    "#     for index, row in df.iterrows():\n",
    "#         percents = []\n",
    "#         corrects = []\n",
    "#         totalanswered_woERRORs = []\n",
    "#         Errors = []\n",
    "        \n",
    "#         # Process each performance column\n",
    "#         for col in performance_columns:\n",
    "#             match = accuracy_pattern.search(str(row[col]))\n",
    "#             if match:\n",
    "#                 percent, correct, totalanswered_woERROR, Error = match.groups()\n",
    "#                 percents.append(float(percent))\n",
    "#                 corrects.append(float(correct))\n",
    "#                 totalanswered_woERRORs.append(float(totalanswered_woERROR))\n",
    "#                 Errors.append(float(Error))\n",
    "        \n",
    "#         # Calculate averages, standard deviations, and confidence intervals\n",
    "#         avg_percent = np.round(np.mean(percents), 2)\n",
    "#         std_dev_percent = np.round(np.std(percents, ddof=1), 2)\n",
    "#         n = len(percents)\n",
    "#         if std_dev_percent != 0:\n",
    "#             ci_percent = stats.norm.interval(0.95, loc=avg_percent, scale=std_dev_percent/np.sqrt(n))\n",
    "#             lci = np.round(ci_percent[0], 2)\n",
    "#             uci = np.round(ci_percent[1], 2)\n",
    "#         else:\n",
    "#             lci = avg_percent\n",
    "#             uci = avg_percent\n",
    "        \n",
    "#         # Calculate 95% CI of mean difference and p-value\n",
    "#         ref_percent = percents[0]  # Assuming the first entry is the reference\n",
    "#         mean_diff = avg_percent - ref_percent\n",
    "#         se_diff = std_dev_percent / np.sqrt(n)\n",
    "#         t_stat = mean_diff / se_diff\n",
    "#         p_value = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=n-1))\n",
    "        \n",
    "#         mean_diff_ci = stats.t.interval(0.95, df=n-1, loc=mean_diff, scale=se_diff)\n",
    "#         mean_diff_ci = np.round(mean_diff_ci, 2)\n",
    "        \n",
    "#         # average answered without ERROR and correct answers X-of-Y\n",
    "#         avg_correct = np.round(np.mean(corrects), 1)\n",
    "        \n",
    "#         totalanswered_woERRORs = [float(x) for x in totalanswered_woERRORs]\n",
    "#         avg_answerdwoerror = np.round(np.mean(totalanswered_woERRORs), 1)\n",
    "#         avg_Error = np.round(np.mean(Errors), 1)\n",
    "        \n",
    "#         # Calculate range of percents\n",
    "#         minpercent = min(percents)\n",
    "#         maxpercent = max(percents)\n",
    "        \n",
    "#         # Update the DataFrame with the calculated statistics\n",
    "#         df.at[index, f'{experimentname}_SummaryAcc'] = (\n",
    "#             f\"{avg_percent}±{std_dev_percent} [95CI: {lci}, {uci}] \"\n",
    "#             f\"[Range: {minpercent}, {maxpercent}] ({avg_correct}-of-{avg_answerdwoerror}; Error: {avg_Error}) \"\n",
    "#             f\"Mean Diff CI: {mean_diff_ci}; p-value={p_value:.3f}   {percents}\" \n",
    "#         )\n",
    "#     df = df.drop(columns=performance_columns)\n",
    "#     return df\n",
    "\n",
    "def average_performance_columns_with_stats(df_acc, experimentname=''):\n",
    "    df = df_acc.copy()\n",
    "    # Regular expression to extract accuracy and correct answers\n",
    "    accuracy_pattern = re.compile(r'(\\d+\\.?\\d*)% \\((\\d+)-of-(\\d+); Error: (\\d+)\\)')\n",
    "    \n",
    "    # Determine performance columns by excluding 'Model Name'\n",
    "    performance_columns = [col for col in df.columns if col != 'Model Name']\n",
    "    \n",
    "    # Reference percentages for comparison\n",
    "    reference_percents = [43.33, 45.0, 43.33]\n",
    "    \n",
    "    # Iterate over the DataFrame to calculate statistics\n",
    "    for index, row in df.iterrows():\n",
    "        percents = []\n",
    "        corrects = []\n",
    "        totalanswered_woERRORs = []\n",
    "        Errors = []\n",
    "        \n",
    "        # Process each performance column\n",
    "        for col in performance_columns:\n",
    "            match = accuracy_pattern.search(str(row[col]))\n",
    "            if match:\n",
    "                percent, correct, totalanswered_woERROR, Error = match.groups()\n",
    "                percents.append(float(percent))\n",
    "                corrects.append(float(correct))\n",
    "                totalanswered_woERRORs.append(float(totalanswered_woERROR))\n",
    "                Errors.append(float(Error))\n",
    "        \n",
    "        # Calculate averages, standard deviations, and confidence intervals\n",
    "        avg_percent = np.round(np.mean(percents), 2)\n",
    "        std_dev_percent = np.round(np.std(percents, ddof=1), 2)\n",
    "        n = len(percents)\n",
    "        if std_dev_percent != 0:\n",
    "            ci_percent = stats.norm.interval(0.95, loc=avg_percent, scale=std_dev_percent/np.sqrt(n))\n",
    "            lci = np.round(ci_percent[0], 2)\n",
    "            uci = np.round(ci_percent[1], 2)\n",
    "        else:\n",
    "            lci = avg_percent\n",
    "            uci = avg_percent\n",
    "            \n",
    "            \n",
    "        try:\n",
    "            # Calculate mean difference using bootstrapping\n",
    "            mean_diff_ci = bootstrap_mean_diff_ci(percents, reference_percents)\n",
    "            mean_diff = np.round(np.mean(percents) - np.mean(reference_percents), 2)\n",
    "\n",
    "            # Perform Wilcoxon signed-rank test\n",
    "            _, p_value = wilcoxon(percents, reference_percents)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR in {experimentname}: {e}\")\n",
    "            mean_diff_ci = mean_diff=p_value= 10000\n",
    "        # Average answered without ERROR and correct answers X-of-Y\n",
    "        avg_correct = np.round(np.mean(corrects), 1)\n",
    "        \n",
    "        totalanswered_woERRORs = [float(x) for x in totalanswered_woERRORs]\n",
    "        avg_answerdwoerror = np.round(np.mean(totalanswered_woERRORs), 1)\n",
    "        avg_Error = np.round(np.mean(Errors), 1)\n",
    "        \n",
    "        # Calculate range of percents\n",
    "        minpercent = min(percents)\n",
    "        maxpercent = max(percents)\n",
    "        \n",
    "        # Update the DataFrame with the calculated statistics\n",
    "        df.at[index, f'{experimentname}_SummaryAcc'] = (\n",
    "            f\"{avg_percent}±{std_dev_percent} [95CI: {lci}, {uci}] [Range: {minpercent}, {maxpercent}] ({avg_correct}-of-{avg_answerdwoerror}; Error: {avg_Error})  {mean_diff} [Mean Diff CI: {mean_diff_ci}] [p-value: {p_value:.4f}]\"\n",
    "        )\n",
    "    df = df.drop(columns=performance_columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_model_accuracy(excel_path):\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(excel_path)\n",
    "    \n",
    "    # Filter columns that end with '_correctness'\n",
    "    correctness_columns = [col for col in df.columns if col.endswith('_correctness')]\n",
    "    \n",
    "    # Prepare a dictionary to hold results\n",
    "    results = {\n",
    "        'Model Name': [],\n",
    "        'Oall_Accu': []\n",
    "    }\n",
    "    \n",
    "    # Analyze each model's performance\n",
    "    for col in correctness_columns:\n",
    "        # Extract the model name (the first component of the column name)\n",
    "        model_name = col.split('_')[0]\n",
    "        \n",
    "        # Count correct and incorrect answers\n",
    "        correct_count = df[col].value_counts().get('correct', 0)\n",
    "        incorrect_count = df[col].value_counts().get('incorrect', 0)\n",
    "        total = df.shape[0]\n",
    "        total_answered = correct_count + incorrect_count\n",
    "        \n",
    "        # Calculate Oall_Accu\n",
    "        accuracy = (correct_count / total) * 100\n",
    "        \n",
    "        \n",
    "        # Format the results\n",
    "        accuracy_str = f'{accuracy:.2f}% ({correct_count}-of-{total_answered}; Error: {total-total_answered})'\n",
    "        \n",
    "        # Append the results\n",
    "        results['Model Name'].append(model_name)\n",
    "        results['Oall_Accu'].append(accuracy_str)\n",
    "\n",
    "        \n",
    "    # Convert results dictionary to a DataFrame for nicer display\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Display the results table\n",
    "    #print(results_df)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming the analyze_model_accuracy function is defined as given previously\n",
    "\n",
    "def analyze_and_merge_multiple_files(file_paths):\n",
    "    # Initialize an empty DataFrame for the final merged results\n",
    "    final_results = pd.DataFrame()\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        # Get the file name without extension for prefixing\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        \n",
    "        # Generate the accuracy table for this file\n",
    "        accuracy_table = analyze_model_accuracy(file_path)\n",
    "        \n",
    "        # Rename columns except for 'Model Name' to include the file name prefix\n",
    "        accuracy_table.rename(columns=lambda x: f\"{file_name}>>>{x}\" if x != \"Model Name\" else x, inplace=True)\n",
    "        \n",
    "        \n",
    "        # Merge the current table with the final_results table\n",
    "        if final_results.empty:\n",
    "            final_results = accuracy_table\n",
    "        else:\n",
    "            # Merge on 'Model Name', keeping all model names encountered so far\n",
    "            final_results = pd.merge(final_results, accuracy_table, on=\"Model Name\", how=\"outer\")\n",
    "    \n",
    "    # Display the final merged table\n",
    "    #print(final_results)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\n",
    "#prompteng_functioncall_raw=r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\E0P4-wseed-temp1.8-3.xlsx\"\n",
    "#prompteng_opencall_raw=r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\E0P4-wseed-temp1.8-2.xlsx\"\n",
    "#prompteng_list=[prompteng_functioncall_raw,prompteng_opencall_raw]\n",
    "#analyze_model_accuracy(prompteng_functioncall_raw)\n",
    "#example = analyze_and_merge_multiple_files(prompteng_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_table_performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_experiments_dic={\n",
    "    'raw':[r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\E0P0-functioncall.xlsx\",\n",
    "    r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\E0P0-functioncall2.xlsx\",\n",
    "    r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\E0P0-functioncall3.xlsx\",],\n",
    "    \n",
    "    'DirectQuestioning':[r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-DirectQuestioning.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-DirectQuestioning-2.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT1024-DirectQuestioning.xlsx\",],\n",
    "    \n",
    "    'OptionAnalysis':[r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-OptionAnalysis.xlsx\",\n",
    "                         r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-OptionAnalysis-2.xlsx\",\n",
    "                         r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT1024-OptionAnalysis.xlsx\",],\n",
    "    \n",
    "    'ChainofThought': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ChainofThought.xlsx\",\n",
    "                       r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ChainofThought-2.xlsx\",\n",
    "                       r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT1024-ChainofThought.xlsx\",],\n",
    "    \n",
    "    'AnswerandJustify': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-AnswerandJustify.xlsx\",\n",
    "                         r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-AnswerandJustify-2.xlsx\",\n",
    "                         r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT1024-AnswerandJustify.xlsx\",],\n",
    "    \n",
    "    'EliminationMethod': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-EliminationMethod.xlsx\",\n",
    "                          r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-EliminationMethod-2.xlsx\",\n",
    "                          r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT1024-EliminationMethod.xlsx\",],\n",
    "    \n",
    "    'ComparativeAnalysis': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ComparativeAnalysis.xlsx\",\n",
    "                            r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ComparativeAnalysis-2.xlsx\",\n",
    "                            r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT1024-ComparativeAnalysis.xlsx\",],\n",
    "    \n",
    "    'ContextualEmbedding': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ContextualEmbedding.xlsx\",\n",
    "                            r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ContextualEmbedding-2.xlsx\",\n",
    "                            r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT1024-ContextualEmbedding.xlsx\",],\n",
    "    \n",
    "    'ConfidenceScoring': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ConfidenceScoring.xlsx\",\n",
    "                          r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ConfidenceScoring-2.xlsx\",\n",
    "                          r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ConfidenceScoring.xlsx\",],\n",
    "    \n",
    "    'ExpertMimicry': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ExpertMimicry.xlsx\",\n",
    "                      r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ExpertMimicry-2.xlsx\",\n",
    "                      r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT1024-ExpertMimicry.xlsx\",],\n",
    "    \n",
    "    'ConsensusTechnique': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ConsensusTechnique.xlsx\",\n",
    "                           r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-ConsensusTechnique-2.xlsx\",\n",
    "                           r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT1024-ConsensusTechnique.xlsx\",],\n",
    "    \n",
    "    'GiveModelTimetoThink': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-GiveModelTimetoThink.xlsx\",\n",
    "                             r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT512-GiveModelTimetoThink-2.xlsx\",\n",
    "                             r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P1-MT1024-GiveModelTimetoThink.xlsx\"],\n",
    "    \n",
    "    'best_prompt_1024': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT1024candid_best1.xlsx\",\n",
    "                    r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT1024candid_best2.xlsx\",\n",
    "                    r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT1024candid_best3.xlsx\",\n",
    "                    r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT1024candid_best4.xlsx\",\n",
    "                    r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT1024candid_best5.xlsx\",\n",
    "                    r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT1024candid_best6.xlsx\",],\n",
    "\n",
    "    'best_prompt_512': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best1.xlsx\",\n",
    "                    r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best2.xlsx\",\n",
    "                    r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best3.xlsx\",\n",
    "                    r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best4.xlsx\",\n",
    "                    r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best5.xlsx\",\n",
    "                    r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best6.xlsx\",],\n",
    "    \n",
    "    'best_prompt1': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best1.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best1-2.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT1024candid_best1.xlsx\"],\n",
    "    \n",
    "    'best_prompt2': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best2.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best2-2.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT1024candid_best2.xlsx\",],\n",
    "    \n",
    "    'best_prompt3': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best3.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best3-2.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT1024candid_best3.xlsx\",],\n",
    "    \n",
    "    'best_prompt4': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best4.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best4-2.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT1024candid_best4.xlsx\",],\n",
    "    \n",
    "    'best_prompt5': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best5.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best5-2.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT1024candid_best5.xlsx\",],\n",
    "    \n",
    "    'best_prompt6': [r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best6.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT512candid_best6-2.xlsx\",\n",
    "                     r\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P2-MT1024candid_best6.xlsx\",],\n",
    "    }\n",
    "\n",
    "\n",
    "allexperiments_list=[]\n",
    "for value in All_experiments_dic.values():\n",
    "    allexperiments_list=allexperiments_list+value\n",
    "\n",
    "allexperiments = analyze_and_merge_multiple_files(allexperiments_list).T\n",
    "allexperiments.columns = allexperiments.iloc[0]\n",
    "allexperiments=allexperiments.drop('Model Name')\n",
    "allexperiments=allexperiments.reset_index()\n",
    "\n",
    "def extract_numbers(s):\n",
    "    # Regular expression patterns to match floating point numbers (including integers)\n",
    "    pattern = r\"[-+]?[0-9]*\\.?[0-9]+\"\n",
    "    matches = re.findall(pattern, s)\n",
    "    \n",
    "    # Convert matches to floats if they exist, otherwise return None\n",
    "    avg = float(matches[0]) if matches else None\n",
    "    sd = float(matches[1]) if len(matches) > 1 else None\n",
    "    lci = float(matches[2]) if len(matches) > 2 else None\n",
    "    uci = float(matches[3]) if len(matches) > 3 else None\n",
    "    \n",
    "    return avg, sd, lci, uci\n",
    "\n",
    "all_table_performances=[]\n",
    "Average_Tables=[]\n",
    "for experiment, experimentlist in All_experiments_dic.items():\n",
    "    table_performances=analyze_and_merge_multiple_files(experimentlist)\n",
    "    all_table_performances.append(table_performances)\n",
    "    table_average=average_performance_columns_with_stats(table_performances, experimentname=experiment)\n",
    "    table_averageT=table_average.T\n",
    "    table_averageF=table_averageT.drop('Model Name')\n",
    "    Average_Tables.append(table_averageF)\n",
    "    \n",
    "allaveragetables=pd.concat(Average_Tables)\n",
    "allaveragetables.columns=['GPT3.5 Accuracy', 'GPT4 Accuracy']\n",
    "\n",
    "\n",
    "for index,row in  allaveragetables.iterrows(): \n",
    "    avg,sd,lci,uci = extract_numbers(row['GPT3.5 Accuracy'])\n",
    "    if avg>45:\n",
    "        improvment=avg-43.89\n",
    "        allaveragetables.at[index, 'Change']=float(improvment)\n",
    "        allaveragetables.at[index, 'Higherthan95UCI']='Yes'\n",
    "    else:\n",
    "        improvment=avg-43.89\n",
    "        allaveragetables.at[index, 'Change']=float(improvment)\n",
    "        allaveragetables.at[index, 'Higherthan95UCI']='No'\n",
    "\n",
    "pd.set_option('display.max_colwidth', 150)  \n",
    "allaveragetables.drop(columns=['GPT4 Accuracy'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_values_avgrange(s):\n",
    "    # Regular expression patterns to match numerical values\n",
    "    avg_pattern = r'(\\d+\\.\\d+)'  # Pattern for average value\n",
    "    range_pattern = r'Range: (\\d+\\.\\d+), (\\d+\\.\\d+)'  # Pattern for range value\n",
    "    \n",
    "    # Find average value\n",
    "    avg_match = re.search(avg_pattern, s)\n",
    "    avg = float(avg_match.group(1)) if avg_match else None\n",
    "    \n",
    "    # Find range values\n",
    "    range_match = re.search(range_pattern, s)\n",
    "    range_start = float(range_match.group(1)) if range_match else None\n",
    "    range_end = float(range_match.group(2)) if range_match else None\n",
    "    \n",
    "    return avg, range_start, range_end\n",
    "\n",
    "allaveragetables=allaveragetables.drop('best_prompt_1024_SummaryAcc')\n",
    "allaveragetables=allaveragetables.drop('best_prompt_512_SummaryAcc')\n",
    "allaveragetables\n",
    "\n",
    "\n",
    "index_to_label_dic= {'raw_SummaryAcc': 'Raw', 'DirectQuestioning_SummaryAcc': 'Direct Questioning',\n",
    "                    'OptionAnalysis_SummaryAcc':'Option Analysis', 'ChainofThought_SummaryAcc':'Chain of Thought',\n",
    "                    'AnswerandJustify_SummaryAcc':'Answer and Justify', 'EliminationMethod_SummaryAcc':'Elimination Method',\n",
    "                    'ComparativeAnalysis_SummaryAcc': 'Comparative Analysis', 'ContextualEmbedding_SummaryAcc': 'Contextual Embedding',\n",
    "                    'ConfidenceScoring_SummaryAcc': 'Confidence Scoring', 'ExpertMimicry_SummaryAcc':'Expert Mimicry',\n",
    "                    'ConsensusTechnique_SummaryAcc':'Consensus Technique', 'GiveModelTimetoThink_SummaryAcc':'Give Model Time to Think',\n",
    "                    'best_prompt1_SummaryAcc':'Candidate 1', 'best_prompt2_SummaryAcc':'Candidate 2',\n",
    "                    'best_prompt3_SummaryAcc':'Candidate 3', 'best_prompt4_SummaryAcc':'Candidate 4',\n",
    "                    'best_prompt5_SummaryAcc':'Candidate 5', 'best_prompt6_SummaryAcc':'Candidate 6'}\n",
    "\n",
    "\n",
    "accuracy_means=[]\n",
    "Lranges=[]\n",
    "Uranges=[]\n",
    "Labels=[]\n",
    "\n",
    "for index, row in allaveragetables.iterrows():\n",
    "    avg, lrange, urange = extract_values_avgrange(row['GPT3.5 Accuracy'])\n",
    "    accuracy_means.append(avg)\n",
    "    Lranges.append(lrange)\n",
    "    Uranges.append(urange)\n",
    "    Labels.append(index_to_label_dic[str(index)])\n",
    "    \n",
    "if len(accuracy_means) == len(Lranges) == len(Uranges) == len(Labels):\n",
    "    print(f'Means: {accuracy_means}')\n",
    "    print(f'Lower ranges: {Lranges}')\n",
    "    print(f'Upper ranges: {Uranges}')\n",
    "    print(f'Labels: {Labels}')\n",
    "    print ('   ✓ ---> All lists have the same length.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt engineering\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for i in range(len(Labels)):\n",
    "    plt.plot([Lranges[i], Uranges[i]], [i, i], color='blue')  # Plotting horizontal lines\n",
    "    plt.plot([accuracy_means[i]], [i], marker='o', markersize=8, color='blue')  # Plotting mean points\n",
    "\n",
    "plt.xlabel('Accuracy (%)', fontsize=14)\n",
    "plt.ylabel('Function', fontsize=14)\n",
    "plt.title('Accuracy of Different Prompt Engineering Techniques', fontsize=16)\n",
    "plt.yticks(range(len(Labels)), Labels, fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max token\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "experiment_settings = [\n",
    "    'Raw', 'Direct Questioning', 'Option Analysis', 'Chain of Thought', \n",
    "    'Answer and Justify', 'Elimination Method', 'Comparative Analysis', \n",
    "    'Contextual Embedding', 'Confidence Scoring', 'Expert Mimicry', \n",
    "    'Consensus Technique', 'Give Model Time to Think', 'Best Prompt (candidate 4)'\n",
    "]\n",
    "\n",
    "max_token_settings = ['Max_token = 512', 'Max_token = 1024', 'Max_token = 512plus']\n",
    "\n",
    "accuracy_raw = [43.33, 50.00, 45.00, 52.54, 48.33, 41.67, 41.67, 48.33, 46.67, 45.00, 38.33, 41.67, 46.67]\n",
    "accuracy_512 = [41.67, 48.33, 43.33, 45.00, 50.00, 46.67, 41.67, 43.33, 53.33, 45.00, 43.33, 46.67, 45.00]\n",
    "accuracy_1024 = [43.33, 48.33, 43.33, 45.00, 50.00, 46.67, 41.67, 43.33, 53.33, 45.00, 43.33, 46.67, 45.00]\n",
    "\n",
    "x = np.arange(len(experiment_settings))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, setting in enumerate(max_token_settings):\n",
    "    plt.barh(x - (bar_width * i), accuracy_raw if i == 0 else (accuracy_512 if i == 1 else accuracy_1024), \n",
    "             height=bar_width, label=setting)\n",
    "\n",
    "plt.xlabel('Accuracy (%)', fontsize=14)\n",
    "plt.ylabel('Experiment Setting', fontsize=14)\n",
    "plt.title('Accuracy of Different Experiment Settings with Varying Max_token Lengths', fontsize=16)\n",
    "plt.yticks(x, experiment_settings, rotation=0, fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E0: all in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define colors for each model\n",
    "gpt3_color = '#1f77b4'  # Blue\n",
    "gpt4_color = '#d62728'  # Red\n",
    "light_blue = '#add8e6'   # Light blue\n",
    "dark_blue = '#00008b'    # Dark blue\n",
    "teal = '#008080'         # Teal\n",
    "arctic = '#a3e4d7'       # Arctic\n",
    "\n",
    "# Figure 1: Accuracy of Different Functions\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "functions = ['OpenAI Open Call', 'OpenAI Function Call', 'Lang Chain']\n",
    "gpt3_api_accuracy = [40.00, 43.33, 40.00]\n",
    "gpt4_api_accuracy = [55.00, 68.33, 66.67]\n",
    "\n",
    "bar_width = 0.4  # Width of the bars\n",
    "\n",
    "plt.barh(np.arange(len(functions)), gpt3_api_accuracy, height=bar_width, color=gpt3_color, label='GPT3-API', zorder=2)\n",
    "plt.barh(np.arange(len(functions)) + bar_width, gpt4_api_accuracy, height=bar_width, color=gpt4_color, label='GPT4-API', zorder=2)\n",
    "\n",
    "plt.xlabel('Accuracy (%)', fontsize=14)\n",
    "plt.ylabel('Function', fontsize=14)\n",
    "plt.title('Accuracy of Different Functions', fontsize=16)\n",
    "plt.yticks(np.arange(len(functions)) + bar_width / 2, functions, fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Figure 2: Accuracy of Different Prompt Engineering Techniques\n",
    "plt.figure(figsize=(12, 13.5))\n",
    "\n",
    "# Data\n",
    "functions = ['Raw', 'Direct Questioning', 'Option Analysis', 'Chain of Thought', 'Answer and Justify', \n",
    "             'Elimination Method', 'Comparative Analysis', 'Contextual Embedding', 'Confidence Scoring',\n",
    "             'Expert Mimicry', 'Consensus Technique', 'Give Model Time to Think', 'Candidate 1', 'Candidate 2', 'Candidate 3', 'Candidate 4', 'Candidate 5', 'Candidate 6']\n",
    "accuracy_means = [43.89, 48.33, 43.89, 47.5, 48.33, 44.17, 42.22, 46.66, 45.56, 45.56, 40.0, 43.89, 38.33, 48.33, 43.89, 48.89, 47.78, 48.89]\n",
    "accuracy_errors = [0.96, 1.67 , 0.96 , 3.54, 1.67 , 3.54 , 0.96 , 2.89 , 1.93 , 0.96 , 2.89, 2.55 , 3.34 ,  0 , 0.96 , 3.47 , 1.92 , 0]\n",
    "\n",
    "plt.errorbar(accuracy_means, np.arange(len(functions)), xerr=accuracy_errors, fmt='o', markersize=8, capsize=5, capthick=2, color=gpt3_color)\n",
    "\n",
    "plt.xlabel('Accuracy (%)', fontsize=14)\n",
    "plt.ylabel('Function', fontsize=14)\n",
    "plt.title('Accuracy of Different Prompt Engineering Techniques', fontsize=16)\n",
    "plt.yticks(np.arange(len(functions)), functions, fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Figure 3: Accuracy of Different Experiment Settings with Varying Max_token Lengths\n",
    "plt.figure(figsize=(12, 13.5))\n",
    "\n",
    "experiment_settings = [\n",
    "    'Raw', 'Direct Questioning', 'Option Analysis', 'Chain of Thought', \n",
    "    'Answer and Justify', 'Elimination Method', 'Comparative Analysis', \n",
    "    'Contextual Embedding', 'Confidence Scoring', 'Expert Mimicry', \n",
    "    'Consensus Technique', 'Give Model Time to Think', 'Best Prompt (candidate 4)'\n",
    "]\n",
    "\n",
    "max_token_settings = ['Max_token = 512', 'Max_token = 1024', 'Max_token = 512plus']\n",
    "\n",
    "accuracy_raw = [43.33, 50.00, 45.00, 52.54, 48.33, 41.67, 41.67, 48.33, 46.67, 45.00, 38.33, 41.67, 46.67]\n",
    "accuracy_512 = [41.67, 48.33, 43.33, 45.00, 50.00, 46.67, 41.67, 43.33, 53.33, 45.00, 43.33, 46.67, 45.00]\n",
    "accuracy_1024 = [43.33, 48.33, 43.33, 45.00, 50.00, 46.67, 41.67, 43.33, 53.33, 45.00, 43.33, 46.67, 45.00]\n",
    "\n",
    "x = np.arange(len(experiment_settings))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Define colors for each bar in the third figure\n",
    "colors_third_figure = [gpt3_color, light_blue, dark_blue]\n",
    "\n",
    "for i, setting in enumerate(max_token_settings):\n",
    "    plt.barh(x - (bar_width * i), accuracy_raw if i == 0 else (accuracy_512 if i == 1 else accuracy_1024), \n",
    "             height=bar_width, label=setting, color=colors_third_figure[i])\n",
    "\n",
    "plt.xlabel('Accuracy (%)', fontsize=14)\n",
    "plt.ylabel('Experiment Setting', fontsize=14)\n",
    "plt.title('Accuracy of Different Experiment Settings with Varying Max_token Lengths', fontsize=16)\n",
    "plt.yticks(x, experiment_settings, rotation=0, fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Figure 4: Accuracy of Different Prompt Settings at Various Temperatures\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "temperatures = ['Temperature = 0', 'Temperature = 0.1', 'Temperature = 0.4', 'Temperature = 0.6', 'Temperature = 0.9', 'Temperature = 1']\n",
    "experiment_settings = ['GPT3.5-API with best prompt', 'GPT3.5-API with raw prompt']\n",
    "\n",
    "accuracy_best_prompt = [45.56, 46.11, 45.55, 42.78, 45.0, 47.78]\n",
    "accuracy_raw_prompt = [43.89, 45.56, 45.0, 42.78, 45.0, 48.33]\n",
    "\n",
    "ci_best_prompt = [(44.47, 46.65), (43.93, 48.29), (42.67, 48.43), (38.85, 46.71), (38.46, 51.53), (44.9, 50.66)]\n",
    "ci_raw_prompt = [(42.8, 44.98), (44.47, 46.65), (43.11, 46.89), (41.69, 43.86), (41.74, 48.27), (45.07, 51.6)]\n",
    "\n",
    "x = np.arange(len(temperatures))\n",
    "\n",
    "# Define colors for each line in the fourth figure\n",
    "colors_fourth_figure = [gpt3_color, arctic]\n",
    "\n",
    "for i, setting in enumerate(experiment_settings):\n",
    "    plt.errorbar(accuracy_best_prompt if i == 0 else accuracy_raw_prompt, x, xerr=np.abs(np.array(ci_best_prompt if i == 0 else ci_raw_prompt).T - np.array(accuracy_best_prompt if i == 0 else accuracy_raw_prompt)), fmt='o', label=setting, capsize=5, capthick=2, color=colors_fourth_figure[i], alpha=0.85)\n",
    "\n",
    "plt.xlabel('Accuracy (%)', fontsize=14)\n",
    "plt.ylabel('Temperature', fontsize=14)\n",
    "plt.title('Accuracy of Different Prompt Settings at Various Temperatures', fontsize=16)\n",
    "plt.yticks(x, temperatures, fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show all figures\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from altair import Padding\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Define colors for each model\n",
    "gpt3_color = '#1f77b4'  # Blue\n",
    "gpt4_color = '#ffa500'  # orange\n",
    "light_blue = '#add8e6'   # Light blue\n",
    "dark_blue = '#00008b'    # Dark blue\n",
    "teal = '#008080'         # Teal\n",
    "arctic = '#a3e4d7'       # Arctic\n",
    "\n",
    "# Create a figure and four subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 15.5), gridspec_kw={'width_ratios': [1, 1], 'height_ratios': [5, 13.5]})\n",
    "\n",
    "# Adjust space between subplots\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.52)\n",
    "\n",
    "\n",
    "# Function to add rectangle with label\n",
    "def add_rectangle(ax, label):\n",
    "    ax.text(-0.475, 1, label, transform=ax.transAxes, fontsize=30, fontweight='bold',\n",
    "        bbox=dict(facecolor='white', edgecolor='black', boxstyle='square,pad=0.15'))\n",
    "\n",
    "#----------------------------------------------------\n",
    "# Figure 1: Accuracy of Different Functions\n",
    "axs[0, 0].set_facecolor('lightgrey')\n",
    "axs[0, 0].grid(True, linestyle='--', linewidth=1, color='white')\n",
    "axs[0, 0].spines['top'].set_visible(False)\n",
    "axs[0, 0].spines['right'].set_visible(False)\n",
    "add_rectangle(axs[0, 0], 'a')\n",
    "\n",
    "functions = ['OpenAI Open Call', 'OpenAI Function Call', 'Lang Chain']\n",
    "gpt3_api_accuracy = [40.00, 43.33, 40.00]\n",
    "gpt4_api_accuracy = [55.00, 68.33, 66.67]\n",
    "\n",
    "bar_width = 0.3  # Width of the bars\n",
    "\n",
    "axs[0, 0].barh(np.arange(len(functions)), gpt3_api_accuracy, height=bar_width, color=gpt3_color, label='GPT3-API', zorder=2)\n",
    "axs[0, 0].barh(np.arange(len(functions)) + bar_width, gpt4_api_accuracy, height=bar_width, color=gpt4_color, label='GPT4-API', zorder=2)\n",
    "\n",
    "axs[0, 0].set_xlabel('Accuracy (%)', fontsize=14)\n",
    "axs[0, 0].set_ylabel('Function', fontsize=14)\n",
    "axs[0, 0].set_title('Accuracy of Different Functions', fontsize=16)\n",
    "axs[0, 0].set_yticks(np.arange(len(functions)) + bar_width / 2)\n",
    "axs[0, 0].set_yticklabels(functions, fontsize=12)\n",
    "axs[0, 0].tick_params(axis='x', labelsize=12)\n",
    "axs[0, 0].legend(fontsize=10)\n",
    "\n",
    "#-----------------------------------------------------------------------------------\n",
    "# Figure 2: Accuracy of Different Prompt Engineering Techniques\n",
    "axs[1, 1].set_facecolor('lightgrey')\n",
    "axs[1, 1].grid(True, linestyle='--', linewidth=1, color='white')\n",
    "axs[1, 1].spines['top'].set_visible(False)\n",
    "axs[1, 1].spines['right'].set_visible(False)\n",
    "\n",
    "add_rectangle(axs[1, 1], 'd')\n",
    "\n",
    "functions = ['Raw', 'Direct Questioning', 'Option Analysis', 'Chain of Thought', 'Answer and Justify', \n",
    "             'Elimination Method', 'Comparative Analysis', 'Contextual Embedding', 'Confidence Scoring',\n",
    "             'Expert Mimicry', 'Consensus Technique', 'Give Model Time to Think', 'Candidate 1', 'Candidate 2', 'Candidate 3', 'Candidate 4', 'Candidate 5', 'Candidate 6']\n",
    "accuracy_means = [43.89, 48.33, 43.89, 47.5, 48.33, 44.17, 42.22, 46.66, 45.56, 45.56, 40.0, 43.89, 38.33, 48.33, 43.89, 48.89, 47.78, 48.89]\n",
    "accuracy_errors = [0.96, 1.67 , 0.96 , 3.54, 1.67 , 3.54 , 0.96 , 2.89 , 1.93 , 0.96 , 2.89, 2.55 , 3.34 ,  0 , 0.96 , 3.47 , 1.92 , 0]\n",
    "\n",
    "axs[1, 1].errorbar(accuracy_means, np.arange(len(functions)), xerr=accuracy_errors, fmt='o', markersize=8, capsize=5, capthick=2, color=gpt3_color)\n",
    "\n",
    "axs[1, 1].set_xlabel('Accuracy (%)', fontsize=14)\n",
    "axs[1, 1].set_ylabel('Function', fontsize=14)\n",
    "axs[1, 1].set_title('Accuracy of Different Prompt Engineering Techniques', fontsize=16)\n",
    "axs[1, 1].set_yticks(np.arange(len(functions)))\n",
    "axs[1, 1].set_yticklabels(functions, fontsize=12)\n",
    "axs[1, 1].tick_params(axis='x', labelsize=12)\n",
    "axs[1, 1].grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "# Figure 3: Accuracy of Different Experiment Settings with Varying Max_token Lengths\n",
    "axs[1, 0].set_facecolor('lightgrey')\n",
    "axs[1, 0].grid(True, linestyle='--', linewidth=1, color='white')\n",
    "axs[1, 0].spines['top'].set_visible(False)\n",
    "axs[1, 0].spines['right'].set_visible(False)\n",
    "\n",
    "add_rectangle(axs[1, 0], 'c')\n",
    "\n",
    "experiment_settings = [\n",
    "    'Raw', 'Direct Questioning', 'Option Analysis', 'Chain of Thought', \n",
    "    'Answer and Justify', 'Elimination Method', 'Comparative Analysis', \n",
    "    'Contextual Embedding', 'Confidence Scoring', 'Expert Mimicry', \n",
    "    'Consensus Technique', 'Give Model Time to Think', 'Best Prompt (candidate 4)'\n",
    "]\n",
    "\n",
    "max_token_settings = ['MT: 512', 'MT: 1024', 'MT: 512plus']\n",
    "\n",
    "accuracy_raw = [43.33, 50.00, 45.00, 52.54, 48.33, 41.67, 41.67, 48.33, 46.67, 45.00, 38.33, 41.67, 46.67]\n",
    "accuracy_512 = [41.67, 48.33, 43.33, 45.00, 50.00, 46.67, 41.67, 43.33, 53.33, 45.00, 43.33, 46.67, 45.00]\n",
    "accuracy_1024 = [43.33, 48.33, 43.33, 45.00, 50.00, 46.67, 41.67, 43.33, 53.33, 45.00, 43.33, 46.67, 45.00]\n",
    "\n",
    "x = np.arange(len(experiment_settings))\n",
    "bar_width = 0.25\n",
    "\n",
    "# Define colors for each bar in the third figure\n",
    "colors_third_figure = [gpt3_color, light_blue, dark_blue]\n",
    "\n",
    "for i, setting in enumerate(max_token_settings):\n",
    "    axs[1, 0].barh(x - (bar_width * i), accuracy_raw if i == 0 else (accuracy_512 if i == 1 else accuracy_1024), \n",
    "             height=bar_width, label=setting, color=colors_third_figure[i])\n",
    "\n",
    "axs[1, 0].set_xlabel('Accuracy (%)', fontsize=14)\n",
    "axs[1, 0].set_ylabel('Experiment Setting', fontsize=14)\n",
    "axs[1, 0].set_title('Accuracy of Various Max_token Lengths', fontsize=16)\n",
    "axs[1, 0].set_yticks(x)\n",
    "axs[1, 0].set_yticklabels(experiment_settings, fontsize=12)\n",
    "axs[1, 0].tick_params(axis='x', labelsize=12)\n",
    "axs[1, 0].legend(fontsize=10)\n",
    "axs[1, 0].grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "#----------------------------------------------------\n",
    "# Figure 4: Accuracy of Different Prompt Settings at Various Temperatures\n",
    "# background and grid\n",
    "axs[0, 1].set_facecolor('lightgrey')\n",
    "axs[0, 1].grid(True, linestyle='--', linewidth=1, color='white')\n",
    "axs[0, 1].spines['top'].set_visible(False)\n",
    "axs[0, 1].spines['right'].set_visible(False)\n",
    "\n",
    "add_rectangle(axs[0, 1], 'b')\n",
    "\n",
    "temperatures = ['Temperature = 0', 'Temperature = 0.1', 'Temperature = 0.4', 'Temperature = 0.6', 'Temperature = 0.9', 'Temperature = 1']\n",
    "experiment_settings = ['best prompt', 'raw prompt']\n",
    "\n",
    "accuracy_best_prompt = [45.56, 46.11, 45.55, 42.78, 45.0, 47.78]\n",
    "accuracy_raw_prompt = [43.89, 45.56, 45.0, 42.78, 45.0, 48.33]\n",
    "\n",
    "ci_best_prompt = [(44.47, 46.65), (43.93, 48.29), (42.67, 48.43), (38.85, 46.71), (38.46, 51.53), (44.9, 50.66)]\n",
    "ci_raw_prompt = [(42.8, 44.98), (44.47, 46.65), (43.11, 46.89), (41.69, 43.86), (41.74, 48.27), (45.07, 51.6)]\n",
    "\n",
    "x = np.arange(len(temperatures))\n",
    "\n",
    "# Define colors for each line in the fourth figure\n",
    "colors_fourth_figure = [gpt3_color, arctic]\n",
    "marker_styles = ['o', '*']  # 'o' for circle, 's' for square\n",
    "\n",
    "for i, setting in enumerate(experiment_settings):\n",
    "    axs[0, 1].errorbar(accuracy_best_prompt if i == 0 else accuracy_raw_prompt, x, xerr=np.abs(np.array(ci_best_prompt if i == 0 else ci_raw_prompt).T - np.array(accuracy_best_prompt if i == 0 else accuracy_raw_prompt)), fmt=marker_styles[i], label=setting, capsize=5, capthick=2, color=colors_fourth_figure[i], alpha=0.85)\n",
    "\n",
    "axs[0, 1].set_xlabel('Accuracy (%)', fontsize=14)\n",
    "axs[0, 1].set_ylabel('Temperature', fontsize=14)\n",
    "axs[0, 1].set_title('Accuracy of Various Temperatures', fontsize=16)\n",
    "axs[0, 1].set_yticks(x)\n",
    "axs[0, 1].set_yticklabels(temperatures, fontsize=12)\n",
    "axs[0, 1].tick_params(axis='x', labelsize=12)\n",
    "axs[0, 1].legend(fontsize=10)\n",
    "axs[0, 1].grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "\n",
    "# save plot\n",
    "plt.savefig('Submit\\\\Figures\\\\E0.png', dpi=400, bbox_inches='tight')\n",
    "plt.savefig('Submit\\\\Figures\\\\E0.jpg', dpi=400, bbox_inches='tight')\n",
    "plt.savefig('Submit\\\\Figures\\\\E0.pdf', dpi=400, bbox_inches='tight')\n",
    "\n",
    "\n",
    "# Show all subplots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E0: Temperature and temp+seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempreature: reading the result\n",
    "tempreture_list= [0, 0.1, 0.2, 0.4, 0.6, 0.8, 0.9, 1, 1.2, 1.4, 1.6, 1.8, 2]\n",
    "\n",
    "tempreture_raw_dic={}\n",
    "tempreture_best_dic={}\n",
    "\n",
    "tempreture_raw_wseed_dic={}\n",
    "tempreture_best_wseed_dic={}\n",
    "\n",
    "for i in tempreture_list:\n",
    "    tempreture_raw_dic[f\"rawtemp_{i}\"] = [\n",
    "        f\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P4-raw-temp{i}.xlsx\",\n",
    "        f\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P4-raw-temp{i}-2.xlsx\",\n",
    "        f\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P4-raw-temp{i}-3.xlsx\",\n",
    "    ]\n",
    "    \n",
    "    for path in tempreture_raw_dic[f\"rawtemp_{i}\"] :\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"This file doesn't exist: {path}\")\n",
    "    \n",
    "    tempreture_best_dic[f\"besttemp_{i}\"] = [\n",
    "        f\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P4-temp{i}.xlsx\",\n",
    "        f\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P4-temp{i}-2.xlsx\",\n",
    "        f\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P4-temp{i}-3.xlsx\"\n",
    "    ]\n",
    "    \n",
    "    for path in tempreture_best_dic[f\"besttemp_{i}\"]  :\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"This file doesn't exist: {path}\")\n",
    "\n",
    "    tempreture_raw_wseed_dic[f\"rawtemp_wseed_{i}\"] = [\n",
    "        f\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P4-raw-wseed-temp{i}.xlsx\",\n",
    "        f\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P4-raw-wseed-temp{i}-2.xlsx\",\n",
    "        f\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P4-raw-wseed-temp{i}-3.xlsx\",\n",
    "    ]\n",
    "    \n",
    "    for path in tempreture_raw_wseed_dic[f\"rawtemp_wseed_{i}\"] :\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"This file doesn't exist: {path}\")\n",
    "    \n",
    "    tempreture_best_wseed_dic[f\"besttemp_wseed_{i}\"] = [\n",
    "        f\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P4-wseed-temp{i}.xlsx\",\n",
    "        f\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P4-wseed-temp{i}-2.xlsx\",\n",
    "        f\"C:\\\\Users\\\\LEGION\\\\Documents\\\\GIT\\\\LLM_answer_GIBoard\\\\DO_NOT_PUBLISH\\\\ACG self asses\\\\E0P4-wseed-temp{i}-3.xlsx\"\n",
    "    ]\n",
    "    \n",
    "    for path in tempreture_best_wseed_dic[f\"besttemp_wseed_{i}\"]  :\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"This file doesn't exist: {path}\")\n",
    "            \n",
    "    \n",
    "\n",
    "# calculate average performances using best prompt\n",
    "best_Average_Tables=[]\n",
    "for experiment, experimentlist in tempreture_best_dic.items():\n",
    "    table_performances=analyze_and_merge_multiple_files(experimentlist)\n",
    "    table_average=average_performance_columns_with_stats(table_performances, experimentname=experiment)\n",
    "    table_averageT=table_average.T\n",
    "    table_averageF=table_averageT.drop('Model Name')\n",
    "    best_Average_Tables.append(table_averageF)\n",
    "    \n",
    "allaveragetables_temp_best=pd.concat(best_Average_Tables)\n",
    "allaveragetables_temp_best.columns=['GPT3 Performance (best)']\n",
    "allaveragetables_temp_best\n",
    "\n",
    "\n",
    "# calculate average performances using raw prompt\n",
    "raw_Average_Tables=[]\n",
    "for experiment, experimentlist in tempreture_raw_dic.items():\n",
    "    table_performances=analyze_and_merge_multiple_files(experimentlist)\n",
    "    table_average=average_performance_columns_with_stats(table_performances, experimentname=experiment)\n",
    "    table_averageT=table_average.T\n",
    "    table_averageF=table_averageT.drop('Model Name')\n",
    "    raw_Average_Tables.append(table_averageF)\n",
    "\n",
    "allaveragetables_temp_raw=pd.concat(raw_Average_Tables)\n",
    "allaveragetables_temp_raw.columns=['GPT3 Performance (raw)']\n",
    "allaveragetables_temp_raw\n",
    "\n",
    "# calculate average performances using best prompt (wseed)\n",
    "best_wseed_Average_Tables=[]\n",
    "for experiment, experimentlist in tempreture_best_wseed_dic.items():\n",
    "    table_performances=analyze_and_merge_multiple_files(experimentlist)\n",
    "    table_average=average_performance_columns_with_stats(table_performances, experimentname=experiment)\n",
    "    table_averageT=table_average.T\n",
    "    table_averageF=table_averageT.drop('Model Name')\n",
    "    best_wseed_Average_Tables.append(table_averageF)\n",
    "    \n",
    "allaveragetables_temp_best_wseed=pd.concat(best_wseed_Average_Tables)\n",
    "allaveragetables_temp_best_wseed.columns=['GPT3 Performance (best)']\n",
    "allaveragetables_temp_best_wseed\n",
    "\n",
    "\n",
    "# calculate average performances using raw prompt (wseed)\n",
    "raw_wseed_Average_Tables=[]\n",
    "for experiment, experimentlist in tempreture_raw_wseed_dic.items():\n",
    "    table_performances=analyze_and_merge_multiple_files(experimentlist)\n",
    "    table_average=average_performance_columns_with_stats(table_performances, experimentname=experiment)\n",
    "    table_averageT=table_average.T\n",
    "    table_averageF=table_averageT.drop('Model Name')\n",
    "    raw_wseed_Average_Tables.append(table_averageF)\n",
    "\n",
    "allaveragetables_temp_raw_wseed=pd.concat(raw_wseed_Average_Tables)\n",
    "allaveragetables_temp_raw_wseed.columns=['GPT3 Performance (raw)']\n",
    "allaveragetables_temp_raw_wseed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "allaveragetables_temp_best_wseed\n",
    "allaveragetables_temp_best\n",
    "allaveragetables_temp_raw\n",
    "allaveragetables_temp_raw_wseed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# temprature simple plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "temperatures = ['Temperature = 0', 'Temperature = 0.1', 'Temperature = 0.4', 'Temperature = 0.6', 'Temperature = 0.9', 'Temperature = 1']\n",
    "experiment_settings = ['GPT3.5-API with best prompt', 'GPT3.5-API with raw prompt']\n",
    "\n",
    "accuracy_best_prompt = [45.56, 46.11, 45.55, 42.78, 45.0, 47.78]\n",
    "accuracy_raw_prompt = [43.89, 45.56, 45.0, 42.78, 45.0, 48.33]\n",
    "\n",
    "# 95% confidence intervals\n",
    "ci_best_prompt = [(44.47, 46.65), (43.93, 48.29), (42.67, 48.43), (38.85, 46.71), (38.46, 51.53), (44.9, 50.66)]\n",
    "ci_raw_prompt = [(42.8, 44.98), (44.47, 46.65), (43.11, 46.89), (41.69, 43.86), (41.74, 48.27), (45.07, 51.6)]\n",
    "\n",
    "x = np.arange(len(temperatures))\n",
    "dy = 0.1  # Adjust this value to control the error bar width\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Plot for Best Prompt\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.errorbar(accuracy_best_prompt, x, xerr=np.abs(np.array(ci_best_prompt).T - np.array(accuracy_best_prompt)), fmt='o', label='Best Prompt', color='b', capsize=5)\n",
    "plt.yticks(x, temperatures, fontsize=12)\n",
    "plt.xlabel('Accuracy (%)', fontsize=14)\n",
    "plt.title('Best Prompt', fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot for Raw Prompt\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.errorbar(accuracy_raw_prompt, x, xerr=np.abs(np.array(ci_raw_prompt).T - np.array(accuracy_raw_prompt)), fmt='o', label='Raw Prompt', color='g', capsize=5)\n",
    "plt.yticks(x, temperatures, fontsize=12)\n",
    "plt.xlabel('Accuracy (%)', fontsize=14)\n",
    "plt.title('Raw Prompt', fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temprature: best-woseed whisker for best\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Define regex patterns\n",
    "pattern_average = r\"(\\d+\\.\\d+)\\s*±\"\n",
    "pattern_std_dev = r\"±(\\d+\\.\\d+)\\s*\\[\"\n",
    "pattern_upper_range = r\"Range:\\s*(\\d+\\.\\d+)\\s*,\"\n",
    "pattern_lower_range = r\",\\s*(\\d+\\.\\d+)\\]\"\n",
    "\n",
    "# Initialize lists to store extracted values\n",
    "best_averages = []\n",
    "best_std_devs = []\n",
    "best_upper_ranges = []\n",
    "best_lower_ranges = []\n",
    "best_row_index = []\n",
    "best_labels=[]\n",
    "\n",
    "# Loop through the DataFrame and extract performance data\n",
    "for idx, row in allaveragetables_temp_best.iterrows():\n",
    "    performance_str = row['GPT3 Performance (best)']\n",
    "    \n",
    "    # Extract performance values using regex\n",
    "    average = re.search(pattern_average, performance_str).group(1)\n",
    "    std_dev = re.search(pattern_std_dev, performance_str).group(1)\n",
    "    upper_range = re.search(pattern_upper_range, performance_str).group(1)\n",
    "    lower_range = re.search(pattern_lower_range, performance_str).group(1)\n",
    "    \n",
    "    # Convert extracted values to float\n",
    "    average = float(average)\n",
    "    std_dev = float(std_dev)\n",
    "    upper_range = float(upper_range)\n",
    "    lower_range = float(lower_range)\n",
    "    \n",
    "    #create labels form row index\n",
    "    prompt_type, temp = re.match(r'(best|raw)temp_([\\d.]+)', idx).groups()\n",
    "    # Modify row index names\n",
    "    if prompt_type == 'best':\n",
    "        prompt_type = 'best'\n",
    "    elif prompt_type == 'raw':\n",
    "        prompt_type = 'raw'\n",
    "        \n",
    "    label = f\"{prompt_type} - temp {temp}\"\n",
    "    \n",
    "    # Append to lists\n",
    "    best_averages.append(average)\n",
    "    best_std_devs.append(std_dev)\n",
    "    best_upper_ranges.append(upper_range)\n",
    "    best_lower_ranges.append(lower_range)\n",
    "    \n",
    "    # Append row index name to the list\n",
    "    best_row_index.append(idx)\n",
    "    best_labels.append(label)\n",
    "\n",
    "# Output the statistics\n",
    "print(\"Averages:\", best_averages)\n",
    "print(\"Standard Deviations:\", best_std_devs)\n",
    "print(\"Upper Ranges:\", best_upper_ranges)\n",
    "print(\"Lower Ranges:\", best_lower_ranges)\n",
    "print(\"Row Index Names:\", best_row_index)\n",
    "print(\"Labels:\", best_labels)\n",
    "\n",
    "if len(best_averages) == len(best_std_devs) == len(best_upper_ranges) == len(best_lower_ranges) == len(best_row_index) == len(best_labels):\n",
    "    print(\" ✓ ---> All lists have the same length.\")\n",
    "else:\n",
    "    print(\" ⚠ ---> Lists have different lengths.\")\n",
    "    \n",
    "    \n",
    "#---------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot average line\n",
    "plt.plot(best_labels, best_averages, marker='o', label='Average')\n",
    "\n",
    "# Plot range (min to max)\n",
    "plt.fill_between(best_labels, best_lower_ranges, best_upper_ranges, alpha=0.3, label='Range (Min to Max)')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Performance %')\n",
    "plt.title('Average-Min-Max Performance across Temperatures (BEST)')\n",
    "plt.legend()\n",
    "\n",
    "# Color\n",
    "plt.grid(True, color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "plt.gca().set_facecolor('lightgray')  # Set background color to gray\n",
    "# Remove top and right spines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allaveragetables_temp_best_wseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temprature: best-wseed whisker for best\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Define regex patterns\n",
    "pattern_average = r\"(\\d+\\.\\d+)\\s*±\"\n",
    "pattern_std_dev = r\"±(\\d+\\.\\d+)\\s*\\[\"\n",
    "pattern_upper_range = r\"Range:\\s*(\\d+\\.\\d+)\\s*,\"\n",
    "pattern_lower_range = r\",\\s*(\\d+\\.\\d+)\\]\"\n",
    "\n",
    "# Initialize lists to store extracted values\n",
    "best_wseed_averages = []\n",
    "best_wseed_std_devs = []\n",
    "best_wseed_upper_ranges = []\n",
    "best_wseed_lower_ranges = []\n",
    "best_wseed_row_index = []\n",
    "best_wseed_labels=[]\n",
    "\n",
    "# Loop through the DataFrame and extract performance data\n",
    "for idx, row in allaveragetables_temp_best_wseed.iterrows():\n",
    "    performance_str = row['GPT3 Performance (best)']\n",
    "    \n",
    "    # Extract performance values using regex\n",
    "    average = re.search(pattern_average, performance_str).group(1)\n",
    "    std_dev = re.search(pattern_std_dev, performance_str).group(1)\n",
    "    upper_range = re.search(pattern_upper_range, performance_str).group(1)\n",
    "    lower_range = re.search(pattern_lower_range, performance_str).group(1)\n",
    "    \n",
    "    # Convert extracted values to float\n",
    "    average = float(average)\n",
    "    std_dev = float(std_dev)\n",
    "    upper_range = float(upper_range)\n",
    "    lower_range = float(lower_range)\n",
    "    \n",
    "    #create labels form row index\n",
    "    prompt_type, temp = re.match(r'(best|raw)temp_wseed_([\\d.]+)', idx).groups()\n",
    "    # Modify row index names\n",
    "    if prompt_type == 'best':\n",
    "        prompt_type = 'best'\n",
    "    elif prompt_type == 'raw':\n",
    "        prompt_type = 'raw'\n",
    "        \n",
    "    label = f\"{prompt_type} - temp {temp}\"\n",
    "    \n",
    "    # Append to lists\n",
    "    best_wseed_averages.append(average)\n",
    "    best_wseed_std_devs.append(std_dev)\n",
    "    best_wseed_upper_ranges.append(upper_range)\n",
    "    best_wseed_lower_ranges.append(lower_range)\n",
    "    \n",
    "    # Append row index name to the list\n",
    "    best_wseed_row_index.append(idx)\n",
    "    best_wseed_labels.append(label)\n",
    "\n",
    "# Output the statistics\n",
    "print(\"Averages:\", best_wseed_averages)\n",
    "print(\"Standard Deviations:\", best_wseed_std_devs)\n",
    "print(\"Upper Ranges:\", best_wseed_upper_ranges)\n",
    "print(\"Lower Ranges:\", best_wseed_lower_ranges)\n",
    "print(\"Row Index Names:\", best_wseed_row_index)\n",
    "print(\"Labels:\", best_wseed_labels)\n",
    "\n",
    "if len(best_wseed_averages) == len(best_wseed_std_devs) == len(best_wseed_upper_ranges) == len(best_wseed_lower_ranges) == len(best_wseed_row_index) == len(best_wseed_labels):\n",
    "    print(\" ✓ ---> All lists have the same length.\")\n",
    "else:\n",
    "    print(\" ⚠ ---> Lists have different lengths.\")\n",
    "    \n",
    "    \n",
    "#---------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot average line\n",
    "plt.plot(best_wseed_labels, best_wseed_averages, marker='o', label='Average')\n",
    "\n",
    "# Plot range (min to max)\n",
    "plt.fill_between(best_wseed_labels, best_wseed_lower_ranges, best_wseed_upper_ranges, alpha=0.3, label='Range (Min to Max)')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Performance %')\n",
    "plt.title('Average-Min-Max Performance across Temperatures (BEST-wseed)')\n",
    "plt.legend()\n",
    "\n",
    "# Color\n",
    "plt.grid(True, color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "plt.gca().set_facecolor('lightgray')  # Set background color to gray\n",
    "# Remove top and right spines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temprature: raw-woseed average-min-max plot\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Define regex patterns\n",
    "pattern_average = r\"(\\d+\\.\\d+)\\s*±\"\n",
    "pattern_std_dev = r\"±(\\d+\\.\\d+)\\s*\\[\"\n",
    "pattern_upper_range = r\"Range:\\s*(\\d+\\.\\d+)\\s*,\"\n",
    "pattern_lower_range = r\",\\s*(\\d+\\.\\d+)\\]\"\n",
    "\n",
    "# Initialize lists to store extracted values\n",
    "raw_averages = []\n",
    "raw_std_devs = []\n",
    "raw_upper_ranges = []\n",
    "raw_lower_ranges = []\n",
    "raw_row_index = []\n",
    "raw_labels=[]\n",
    "\n",
    "# Loop through the DataFrame and extract performance data\n",
    "for idx, row in allaveragetables_temp_raw.iterrows():\n",
    "    performance_str = row['GPT3 Performance (raw)']\n",
    "    \n",
    "    # Extract performance values using regex\n",
    "    average = re.search(pattern_average, performance_str).group(1)\n",
    "    std_dev = re.search(pattern_std_dev, performance_str).group(1)\n",
    "    upper_range = re.search(pattern_upper_range, performance_str).group(1)\n",
    "    lower_range = re.search(pattern_lower_range, performance_str).group(1)\n",
    "    \n",
    "    # Convert extracted values to float\n",
    "    average = float(average)\n",
    "    std_dev = float(std_dev)\n",
    "    upper_range = float(upper_range)\n",
    "    lower_range = float(lower_range)\n",
    "    \n",
    "    #create labels form row index\n",
    "    prompt_type, temp = re.match(r'(best|raw)temp_([\\d.]+)', idx).groups()\n",
    "    # Modify row index names\n",
    "    if prompt_type == 'best':\n",
    "        prompt_type = 'best'\n",
    "    elif prompt_type == 'raw':\n",
    "        prompt_type = 'raw'\n",
    "        \n",
    "    label = f\"{prompt_type} - temp {temp}\"\n",
    "    \n",
    "    # Append to lists\n",
    "    raw_averages.append(average)\n",
    "    raw_std_devs.append(std_dev)\n",
    "    raw_upper_ranges.append(upper_range)\n",
    "    raw_lower_ranges.append(lower_range)\n",
    "    \n",
    "    # Append row index name to the list\n",
    "    raw_row_index.append(idx)\n",
    "    raw_labels.append(label)\n",
    "\n",
    "# Output the statistics\n",
    "print(\"Averages:\", raw_averages)\n",
    "print(\"Standard Deviations:\", raw_std_devs)\n",
    "print(\"Upper Ranges:\", raw_upper_ranges)\n",
    "print(\"Lower Ranges:\", raw_lower_ranges)\n",
    "print(\"Row Index Names:\", raw_row_index)\n",
    "print(\"Labels: \", raw_labels)\n",
    "\n",
    "if len(raw_averages) == len(raw_std_devs) == len(raw_upper_ranges) == len(raw_lower_ranges) == len(raw_row_index) == len(raw_labels):\n",
    "    print(\" ✓ ---> All lists have the same length.\")\n",
    "else:\n",
    "    print(\" ⚠ ---> Lists have different lengths.\")\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot average line\n",
    "plt.plot(raw_labels, raw_averages, marker='o', label='Average', color='red')\n",
    "\n",
    "# Plot range (min to max)\n",
    "plt.fill_between(raw_labels, raw_lower_ranges, raw_upper_ranges, alpha=0.3, label='Range (Min to Max)', color='red')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Performance %')\n",
    "plt.title('Average, Min, and Max Performance across Temperatures (RAW-woseed)')\n",
    "plt.legend()\n",
    "# Color\n",
    "plt.grid(True, color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "plt.gca().set_facecolor('lightgray')  # Set background color to gray\n",
    "# Remove top and right spines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temprature: raw-wseed average-min-max plot\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Define regex patterns\n",
    "pattern_average = r\"(\\d+\\.\\d+)\\s*±\"\n",
    "pattern_std_dev = r\"±(\\d+\\.\\d+)\\s*\\[\"\n",
    "pattern_upper_range = r\"Range:\\s*(\\d+\\.\\d+)\\s*,\"\n",
    "pattern_lower_range = r\",\\s*(\\d+\\.\\d+)\\]\"\n",
    "\n",
    "# Initialize lists to store extracted values\n",
    "raw_wseed_averages = []\n",
    "raw_wseed_std_devs = []\n",
    "raw_wseed_upper_ranges = []\n",
    "raw_wseed_lower_ranges = []\n",
    "raw_wseed_row_index = []\n",
    "raw_wseed_labels=[]\n",
    "\n",
    "# Loop through the DataFrame and extract performance data\n",
    "for idx, row in allaveragetables_temp_raw_wseed.iterrows():\n",
    "    performance_str = row['GPT3 Performance (raw)']\n",
    "    \n",
    "    # Extract performance values using regex\n",
    "    average = re.search(pattern_average, performance_str).group(1)\n",
    "    std_dev = re.search(pattern_std_dev, performance_str).group(1)\n",
    "    upper_range = re.search(pattern_upper_range, performance_str).group(1)\n",
    "    lower_range = re.search(pattern_lower_range, performance_str).group(1)\n",
    "    \n",
    "    # Convert extracted values to float\n",
    "    average = float(average)\n",
    "    std_dev = float(std_dev)\n",
    "    upper_range = float(upper_range)\n",
    "    lower_range = float(lower_range)\n",
    "    \n",
    "    #create labels form row index\n",
    "    prompt_type, temp = re.match(r'(best|raw)temp_wseed_([\\d.]+)', idx).groups()\n",
    "    # Modify row index names\n",
    "    if prompt_type == 'best':\n",
    "        prompt_type = 'best'\n",
    "    elif prompt_type == 'raw':\n",
    "        prompt_type = 'raw'\n",
    "        \n",
    "    label = f\"{prompt_type} - temp {temp}\"\n",
    "    \n",
    "    # Append to lists\n",
    "    raw_wseed_averages.append(average)\n",
    "    raw_wseed_std_devs.append(std_dev)\n",
    "    raw_wseed_upper_ranges.append(upper_range)\n",
    "    raw_wseed_lower_ranges.append(lower_range)\n",
    "    \n",
    "    # Append row index name to the list\n",
    "    raw_wseed_row_index.append(idx)\n",
    "    raw_wseed_labels.append(label)\n",
    "\n",
    "# Output the statistics\n",
    "print(\"Averages:\", raw_wseed_averages)\n",
    "print(\"Standard Deviations:\", raw_wseed_std_devs)\n",
    "print(\"Upper Ranges:\", raw_wseed_upper_ranges)\n",
    "print(\"Lower Ranges:\", raw_wseed_lower_ranges)\n",
    "print(\"Row Index Names:\", raw_wseed_row_index)\n",
    "print(\"Labels: \", raw_wseed_labels)\n",
    "\n",
    "if len(raw_wseed_averages) == len(raw_wseed_std_devs) == len(raw_wseed_upper_ranges) == len(raw_wseed_lower_ranges) == len(raw_wseed_row_index) == len(raw_wseed_labels):\n",
    "    print(\" ✓ ---> All lists have the same length.\")\n",
    "else:\n",
    "    print(\" ⚠ ---> Lists have different lengths.\")\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot average line\n",
    "plt.plot(raw_wseed_labels, raw_wseed_averages, marker='o', label='Average', color='red')\n",
    "\n",
    "# Plot range (min to max)\n",
    "plt.fill_between(raw_wseed_labels, raw_wseed_lower_ranges, raw_wseed_upper_ranges, alpha=0.3, label='Range (Min to Max)', color='red')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Performance %')\n",
    "plt.title('Average, Min, and Max Performance across Temperatures (RAW wseed)')\n",
    "plt.legend()\n",
    "# Color\n",
    "plt.grid(True, color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "plt.gca().set_facecolor('lightgray')  # Set background color to gray\n",
    "# Remove top and right spines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mixing plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 12), sharey=True)  # Create a figure with 1 row and 2 columns\n",
    "\n",
    "# First plot (raw)------------------------------------\n",
    "raw_woseed_color='#8ab2c0'\n",
    "axs[0,0].plot(raw_labels, raw_averages, marker='o', label='Average', color=raw_woseed_color)\n",
    "axs[0,0].fill_between(raw_labels, raw_lower_ranges, raw_upper_ranges, alpha=0.4, label='Range (Min to Max)', color=raw_woseed_color)\n",
    "axs[0,0].set_xlabel('')\n",
    "axs[0,0].set_ylabel('Accuracy %')\n",
    "axs[0,0].set_title('Average-Min-Max Plot using Raw Prompt without Determining Seed')\n",
    "axs[0,0].legend()\n",
    "axs[0,0].grid(True, color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "axs[0,0].set_facecolor('lightgray')  # Set background color to gray\n",
    "axs[0,0].spines['top'].set_visible(False)  # Remove top spine\n",
    "axs[0,0].spines['right'].set_visible(False)  # Remove right spine\n",
    "axs[0,0].tick_params(axis='x', rotation=90)  # Rotate x-axis labels\n",
    "# Highlight maximum value with a red border\n",
    "max_index = raw_averages.index(max(raw_averages))\n",
    "axs[0,0].plot(raw_labels[max_index], raw_averages[max_index], marker='o', markersize=8, markeredgecolor='red', markeredgewidth=1, color=raw_woseed_color)\n",
    "\n",
    "\n",
    "\n",
    "# Second plot (best-woseed)----------------------------------\n",
    "best_woseed_color='#00ace6'\n",
    "axs[0,1].plot(best_labels, best_averages, marker='o', label='Average', color=best_woseed_color)\n",
    "axs[0,1].fill_between(best_labels, best_lower_ranges, best_upper_ranges, alpha=0.3, label='Range (Min to Max)', color=best_woseed_color)\n",
    "axs[0,1].set_xlabel('')\n",
    "axs[0,1].set_title('Average-Min-Max Plot using Best Prompt without Determining Seed')\n",
    "axs[0,1].legend()\n",
    "axs[0,1].grid(True, color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "axs[0,1].set_facecolor('lightgray')  # Set background color to gray\n",
    "axs[0,1].spines['top'].set_visible(False)  # Remove top spine\n",
    "axs[0,1].spines['right'].set_visible(False)  # Remove right spine\n",
    "axs[0,1].tick_params(axis='x', rotation=90)  # Rotate x-axis labels\n",
    "# Highlight maximum value with a red border\n",
    "max_index = best_averages.index(max(best_averages))\n",
    "axs[0,1].plot(best_labels[max_index], best_averages[max_index], marker='o', markersize=8, markeredgecolor='red', markeredgewidth=1,color=best_woseed_color)\n",
    "\n",
    "# Third plot (raw-wseed)-------------------------------------\n",
    "raw_wseed_color='#6BA17F'\n",
    "axs[1,0].plot(raw_wseed_labels, raw_wseed_averages, marker='o', label='Average', color=raw_wseed_color)\n",
    "axs[1,0].fill_between(raw_wseed_labels, raw_wseed_lower_ranges, raw_wseed_upper_ranges, alpha=0.4, label='Range (Min to Max)', color=raw_wseed_color)\n",
    "axs[1,0].set_xlabel('')\n",
    "axs[1,0].set_ylabel('Accuracy %')\n",
    "axs[1,0].set_title('Average-Min-Max Plot using Raw Prompt with Determined Seed')\n",
    "axs[1,0].legend()\n",
    "axs[1,0].grid(True, color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "axs[1,0].set_facecolor('lightgray')  # Set background color to gray\n",
    "axs[1,0].spines['top'].set_visible(False)  # Remove top spine\n",
    "axs[1,0].spines['right'].set_visible(False)  # Remove right spine\n",
    "axs[1,0].tick_params(axis='x', rotation=90)  # Rotate x-axis labels\n",
    "# Highlight maximum value with a red border\n",
    "max_index = raw_wseed_averages.index(max(raw_wseed_averages))\n",
    "axs[1,0].plot(raw_wseed_labels[max_index], raw_wseed_averages[max_index], marker='o', markersize=8, markeredgecolor='red', markeredgewidth=1, color=raw_wseed_color)\n",
    "\n",
    "\n",
    "\n",
    "# Foruth plot (best-wseed)-------------------------------------------------\n",
    "best_wseed_color='#20CE60'\n",
    "axs[1,1].plot(best_wseed_labels, best_wseed_averages, marker='o', label='Average', color=best_wseed_color)\n",
    "axs[1,1].fill_between(best_wseed_labels, best_wseed_lower_ranges, best_wseed_upper_ranges, alpha=0.3, label='Range (Min to Max)', color=best_wseed_color)\n",
    "axs[1,1].set_xlabel('')\n",
    "axs[1,1].set_title('Average-Min-Max Plot using Best Prompt with Determined Seed')\n",
    "axs[1,1].legend()\n",
    "axs[1,1].grid(True, color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "axs[1,1].set_facecolor('lightgray')  # Set background color to gray\n",
    "axs[1,1].spines['top'].set_visible(False)  # Remove top spine\n",
    "axs[1,1].spines['right'].set_visible(False)  # Remove right spine\n",
    "axs[1,1].tick_params(axis='x', rotation=90)  # Rotate x-axis labels\n",
    "# Highlight maximum value with a red border\n",
    "max_index = best_wseed_averages.index(max(best_wseed_averages))\n",
    "axs[1,1].plot(best_wseed_labels[max_index], best_wseed_averages[max_index], marker='o', markersize=8,color=best_wseed_color, markeredgecolor='red', markeredgewidth=1)\n",
    "\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "\n",
    "# Save figure\n",
    "fig_path=r'C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figures\\E0_Temp'\n",
    "plt.savefig(f'{fig_path}.pdf', dpi=500, bbox_inches='tight')  # Save as PDF with higher resolution\n",
    "#plt.savefig(f'{fig_path}.png', dpi=500, bbox_inches='tight')  # Save as PNG with higher resolution\n",
    "#plt.savefig(f'{fig_path}.jpg', dpi=500, bbox_inches='tight')  # Save as JPG with higher resolution\n",
    "\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1 raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def interactive_count_values(df, columns, replacement_dictionary=None):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise ValueError(\"The input df must be a pandas DataFrame.\")\n",
    "    if not all(isinstance(col, str) and col in df.columns for col in columns):\n",
    "        raise ValueError(\"All elements of columns must be valid column names in the DataFrame.\")\n",
    "    if replacement_dictionary is not None and not isinstance(replacement_dictionary, dict):\n",
    "        raise ValueError(\"replacement_dictionary must be a dictionary.\")\n",
    "    \n",
    "    counts_dict = {}\n",
    "    replacement_dict = replacement_dictionary if replacement_dictionary is not None else {}\n",
    "    \n",
    "    for col in columns:\n",
    "        counts = df[col].value_counts(normalize=True) * 100\n",
    "        print(f\"Counts for column '{col}':\")\n",
    "        print(counts)\n",
    "        \n",
    "        replacements = {}\n",
    "        for value, percentage in counts.items():\n",
    "            if value in replacement_dict:\n",
    "                replacements[value] = replacement_dict[value]\n",
    "            else:\n",
    "                replacement = input(f\"Enter replacement for '{value}': \")\n",
    "                replacement_dict[value] = replacement\n",
    "                if replacement:  # Only add non-empty replacements\n",
    "                    replacements[value] = replacement\n",
    "                \n",
    "        if replacements:\n",
    "            df[col].replace(replacements)\n",
    "\n",
    "        counts_dict[col] = df[col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    return counts_dict, replacement_dict\n",
    "\n",
    "# Load data from Excel file (replace 'excel_file_path' with your file path)\n",
    "excel_file_path = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\__Milestone Datasets\\E1_Bestprompt-2022_all_Final_E2_manualevaluatedcompleted_Final_E2_Final.xlsx\"\n",
    "df = pd.read_excel(excel_file_path)\n",
    "df = df.apply(lambda x: re.sub('^Resolved: ', '', x) if isinstance(x, str) else x)\n",
    "\n",
    "# Identify the specific columns that we're interested in\n",
    "correctness_columns = [col for col in df.columns if col.endswith('_correctness')]\n",
    "\n",
    "\n",
    "replacement_dictionary={'Incorrect': 'Incorrect', 'incorrect': 'Incorrect', 'Correct': 'Correct', 'correct': 'Correct', 'EOP': 'EOP', '2OP': '2OP', 'NOP': 'NOP', 'No answer': 'NoA', 'NoA': 'NoA'}\n",
    "\n",
    "\n",
    "# Call the function to interactively count values and handle replacements\n",
    "model_correctness_counts, replacements = interactive_count_values(df, correctness_columns, replacement_dictionary=replacement_dictionary)\n",
    "\n",
    "# Print the final counted data for each column\n",
    "print(\"\\nFinal counted data for each column:\")\n",
    "for col, counts in model_correctness_counts.items():\n",
    "    print(f\"Counts for column '{col}':\")\n",
    "    print(counts)\n",
    "\n",
    "# Print the replacement dictionary\n",
    "print(\"\\nReplacement dictionary:\")\n",
    "print(replacements)\n",
    "\n",
    "\n",
    "# Transform the dictionary into a DataFrame\n",
    "df_plot = pd.DataFrame(model_correctness_counts).T.fillna(0)\n",
    "\n",
    "# Plotting\n",
    "ax = df_plot.plot(kind='bar', stacked=True, figsize=(14, 6))\n",
    "\n",
    "ax.set_ylabel('Percent')\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_title('Correctness Percentage Among Models')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "label_dic={\n",
    "    # Web\n",
    "        #GPTs\n",
    "            'GPT3.5Web_correctness':\"GPT-3.5\",\n",
    "            'GPT4Web_correctness':\"GPT-4\", \n",
    "        #Claude\n",
    "        'ClaudeHiakuWeb-raw_correctness':\"Claude3-Hiaku\", \n",
    "        'ClaudeSonnetWeb-raw_correctness': \"Claude3-Sonnet\",\n",
    "        'ClaudeOpuWeb-raw_correctness':\"Claude3-Opus\", \n",
    "\n",
    "        # Gemini\n",
    "        'GeminiWeb-raw_correctness':\"Gemini\",\n",
    "        'GeminiAdvancedWeb-raw_correctness':\"GeminiAdvanced\", \n",
    "        \n",
    "    \" \": \" \",\n",
    "    # Poe\n",
    "    'Mixtral-8x7B-Poe_correctness':\"Mixtral-8x7b\",\n",
    "    'MistralLarge-Poe_correctness':\"Mistral-Large\",\n",
    "    'Mistral-7B-T_correctness':\"Mistralv2-7b\",\n",
    "        'Llama27B-Poe_correctness': \"Llama2-7b\", \n",
    "        'Llama-2-13b_correctness': 'Llama2-13b',\n",
    "        'Llama270B-Poe_correctness':\"Llama2-70b\",\n",
    "        'Llama3-8b-Poe_correctness':'Llama3-8b',\n",
    "        'Llama3-70b-Poe_correctness':'Llama3-70b',\n",
    "        'gemma2-9b-it_correctness': \"Gemma2-9b\",\n",
    "        \n",
    "        \n",
    "    \n",
    "    \"  \": \"  \",\n",
    "    #API\n",
    "    'gpt-3.5-turbo-0125_correctness': \"GPT-3.5 \",\n",
    "    'gpt-4-0613_correctness':\"GPT-4 \",\n",
    "    'gpt-4o-2024-05-13_correctness': \"GPT-4o \",\n",
    "    'gpt-4o-mini-2024-07-18_correctness':\"GPT-4omini\",\n",
    "    \n",
    "    'claude-3-sonnet-20240229_correctness':\"Claude3-Haiku \",    \n",
    "    'claude-3-haiku-20240307_correctness':\"Claude3-Sonnet \",    \n",
    "    'claude-3-opus-20240229_correctness':\"Claude3-Opus \",\n",
    "    'claude-3-5-sonnet-20240620_correctness': \"Claude3.5-Sonnet \",\n",
    "\n",
    "\n",
    "\n",
    "    \"   \": \"   \",\n",
    "    # Local\n",
    "    \"llama3-8b-Q8_correctness\": \"Llama3-8b-Q8\", \n",
    "    'openbioLLM-7B-Q8_correctness': \"OpenBioLLM-8b-Q8\", \n",
    "    'llama2-13B-Q5KM_correctness':\"Llama2-13b-Q5KM\", \n",
    "    'llama2-7B-Q8_correctness':\"Llama2-7b-Q8-Local\",\n",
    "    'medicine-chat-Q8_correctness':\"medicineLLM-7b-Q8\",\n",
    "    'mistral-instruct-v2-Q8_correctness':\"Mistralv2-7b-Q8\",\n",
    "    'phi3-3b-Q16_correctness':'Phi3-3b-Q16',\n",
    "    'Phi3-medium14b-Q6_correctness':'Phi3-14b-Q6',\n",
    "    'Gemma2-9b-Q8_correctness': \"Gemmat2-9b-Q8\",\n",
    "    }\n",
    "\n",
    "\n",
    "replacement_dictionary = {'Incorrect': 'Incorrect', 'incorrect': 'Incorrect', 'Correct': 'Correct', 'correct': 'Correct', 'EOP': 'EOP', '2OP': '2OP', 'NOP': 'NOP', 'No answer': 'NoA', 'NoA': 'NoA'}\n",
    "color_map= {\n",
    "        \"Correct\": \"#008000\",  # green\n",
    "        \"2OP\": \"#90EE90\",  # light green\n",
    "        \"EOP\": \"#D3D3D3\",  # light grey\n",
    "        \"NOP\": \"#A9A9A9\",  # grey\n",
    "        \"NoA\": \"#ffa500\",\n",
    "        \"Incorrect\": \"#FF0000\"  # red\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic={\n",
    "    # Web\n",
    "        #GPTs\n",
    "    'GPT3.5Web_correctness':\"GPT-3.5 Web\",\n",
    "    'gpt-3.5-turbo-0125_correctness': \"GPT-3.5 API\",\n",
    "    'GPT4Web_correctness':\"GPT-4 Web\", \n",
    "    'gpt-4-0613_correctness':\"GPT-4 API\",\n",
    "    'gpt-4o-2024-05-13_correctness': \"GPT-4o API\",\n",
    "    'gpt-4o-mini-2024-07-18_correctness':\"GPT-4omini API\",           \n",
    "    r\" \": r\" \",        \n",
    "            \n",
    "            \n",
    "    #Claude\n",
    "    'ClaudeHiakuWeb-raw_correctness':\"Claude3-Hiaku Web\",\n",
    "    'claude-3-sonnet-20240229_correctness':\"Claude3-Haiku API\",  \n",
    "    'ClaudeSonnetWeb-raw_correctness': \"Claude3-Sonnet Web\",\n",
    "    'claude-3-haiku-20240307_correctness':\"Claude3-Sonnet API\", \n",
    "    'ClaudeOpuWeb-raw_correctness':\"Claude3-Opus Web\", \n",
    "    'claude-3-opus-20240229_correctness':\"Claude3-Opus API\",\n",
    "    'claude-3-5-sonnet-20240620_correctness': \"Claude3.5-Sonnet API\",\n",
    "    r\"  \": r\"  \",    \n",
    "        \n",
    "    \n",
    "    # Poe\n",
    "    'Mistral-7B-T_correctness':\"Mistralv2-7b Poe\",\n",
    "    'mistral-instruct-v2-Q8_correctness':\"Mistralv2-7b-Q8 Local\",\n",
    "    'MistralLarge-Poe_correctness':\"Mistral-Large Poe\",\n",
    "    'Mixtral-8x7B-Poe_correctness':\"Mixtral-8x7b Poe\",\n",
    "    \n",
    "    r\"     \": r\"     \",\n",
    "    'Llama27B-Poe_correctness': \"Llama2-7b Poe\", \n",
    "    'llama2-7B-Q8_correctness':\"Llama2-7b-Q8 Local\",\n",
    "    'medicine-chat-Q8_correctness':\"medicineLLM-7b-Q8 Local\",\n",
    "    'Llama-2-13b_correctness': 'Llama2-13b Poe',\n",
    "    'llama2-13B-Q5KM_correctness':\"Llama2-13b-Q5 Local\", \n",
    "    'Llama270B-Poe_correctness':\"Llama2-70b Poe\",\n",
    "    'Llama3-8b-Poe_correctness':'Llama3-8b Poe',\n",
    "    \"llama3-8b-Q8_correctness\": \"Llama3-8b-Q8 Local\", \n",
    "    'openbioLLM-7B-Q8_correctness': \"OpenBioLLM-8b-Q8 Local\", \n",
    "    'Llama3-70b-Poe_correctness':'Llama3-70b Poe',\n",
    "    \"Llama3.1-8B_correctness\": \"Llama3.1-8b Poe\",\n",
    "    \"Llama3.1-70B_correctness\": \"Llama3.1-70b Poe\",\n",
    "    \"Llama3.1-405B_correctness\": \"Llama3.1-405b Poe\",\n",
    "    \n",
    "    r\"       \": r\"       \",\n",
    "    \n",
    "        # Gemini\n",
    "    'GeminiWeb-raw_correctness':\"Gemini Web\",\n",
    "    'GeminiAdvancedWeb-raw_correctness':\"GeminiAdvanced Web\", \n",
    "    'gemma2-9b-it_correctness': \"Gemma2-9b Poe\",\n",
    "    'Gemma2-9b-Q8_correctness': \"Gemma2-9b-Q8 Local\",\n",
    "    \"gemma2-27b-it_correctness\": \"Gemma2-27b Poe\",\n",
    "    r\"    \": r\"    \",\n",
    "\n",
    "    'phi3-3b-Q16_correctness':'Phi3-3b-Q16 Local',\n",
    "    'Phi3-medium14b-Q6_correctness':'Phi3-14b-Q6 Local',\n",
    "    }\n",
    "\n",
    "\n",
    "replacement_dictionary = {'Incorrect': 'Incorrect', 'incorrect': 'Incorrect', 'Correct': 'Correct', 'correct': 'Correct', 'EOP': 'EOP', '2OP': '2OP', 'NOP': 'NOP', 'No answer': 'Error', 'NoA': 'Error'}\n",
    "color_map= {\n",
    "        \"Correct\": \"#008000\",  # green\n",
    "        \"2OP\": \"#90EE90\",  # light green\n",
    "        \"EOP\": \"#D3D3D3\",  # light grey\n",
    "        \"NOP\": \"#A9A9A9\",  # grey\n",
    "        \"Error\": \"#ffa500\",\n",
    "        \"Incorrect\": \"#FF0000\"  # red\n",
    "    }\n",
    "\n",
    "excel_file_path = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\__Milestone Datasets\\E1_Bestprompt-2022_all_Final_E2_manualevaluatedcompleted_Final_E2_Final.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def interactive_count_values(df, columns,replacement_dictionary=None):\n",
    "    # Dictionary to store counts of unique values for each column\n",
    "    counts_dict = {}\n",
    "    \n",
    "    # Dictionary to store replacements to avoid asking again\n",
    "    if replacement_dictionary is None:\n",
    "        replacement_dict = {}\n",
    "    else: \n",
    "        replacement_dict=replacement_dictionary\n",
    "    \n",
    "    for col in columns:\n",
    "        # Calculate counts of unique values for this column\n",
    "        counts = df[col].value_counts(normalize=True) * 100  # Convert to percentages\n",
    "        \n",
    "        # Display counts and ask for replacements\n",
    "        #print(f\"Counts for column '{col}':\")\n",
    "        #print(counts)\n",
    "        for value, percentage in counts.items():\n",
    "            # Check if a replacement is already stored\n",
    "            if value in replacement_dict:\n",
    "                replacement = replacement_dict[value]\n",
    "            else:\n",
    "                replacement = input(f\"Enter replacement for '{value}': \")\n",
    "                # Store replacement to avoid asking again\n",
    "                replacement_dict[value] = replacement\n",
    "                \n",
    "            # Replace value if needed\n",
    "            if replacement != \"\":\n",
    "                df[col] = df[col].replace({value: replacement})\n",
    "        \n",
    "        # Store counts for this column in the dictionary\n",
    "        counts_dict[col] = df[col].value_counts(normalize=True) * 100  # Update counts after replacements\n",
    "    \n",
    "    return counts_dict, replacement_dict\n",
    "\n",
    "def draw_line(ax, pos, length, text):\n",
    "    line_y = data.max() + 10  # slightly above the highest bar\n",
    "    ax.annotate(\n",
    "        text, \n",
    "        xy=(pos, line_y + 5), \n",
    "        xytext=(0, 0), textcoords=\"offset points\",\n",
    "        ha='center', va='bottom'\n",
    "    )\n",
    "    ax.plot([pos, pos + length], [line_y, line_y], color='black')\n",
    "\n",
    "# Load data from Excel file (replace 'excel_file_path' with your file path)\n",
    "excel_file_path = excel_file_path\n",
    "df = pd.read_excel(excel_file_path)\n",
    "df = df.apply(lambda x: re.sub('^Resolved: ', '', x) if isinstance(x, str) else x)\n",
    "\n",
    "# Identify the specific columns that we're interested in\n",
    "correctness_columns = [col for col in df.columns if col.endswith('_correctness')]\n",
    "\n",
    "replacement_dictionary=replacement_dictionary\n",
    "\n",
    "\n",
    "\n",
    "# Call the function to interactively count values and handle replacements\n",
    "model_correctness_counts, replacements = interactive_count_values(df, correctness_columns, replacement_dictionary=replacement_dictionary)\n",
    "\n",
    "# Print the final counted data for each column\n",
    "#print(\"\\nFinal counted data for each column:\")\n",
    "#for col, counts in model_correctness_counts.items():\n",
    "#    print(f\"Counts for column '{col}':\")\n",
    "#    print(counts)\n",
    "\n",
    "# Print the replacement dictionary\n",
    "print(\"\\nReplacement dictionary:\")\n",
    "print(replacements)\n",
    "\n",
    "\n",
    "# Transform the dictionary into a DataFrame\n",
    "df_plot = pd.DataFrame(model_correctness_counts).T.fillna(0)\n",
    "\n",
    "color_map = color_map\n",
    "# Reordering columns to match color map\n",
    "desired_columns = [key for key in color_map]\n",
    "valid_columns = [col for col in desired_columns if col in df_plot.columns]\n",
    "missing_columns = [col for col in desired_columns if col not in df_plot.columns]\n",
    "for col in missing_columns:\n",
    "    df_plot[col] = 0\n",
    "df_plot = df_plot[desired_columns]\n",
    "\n",
    "\n",
    "# labeling and sorting indexes (models) -------------------------------\n",
    "label_dic=label_dic\n",
    "\n",
    "invalid_indexes_in_desireddic = [key for key, value in label_dic.items() if key not in df_plot.index]\n",
    "print(f\"WARNING ! These labels you have in your label_dic are not valid in the df.index Edit the label or remove it if you added this by mistake\\n {invalid_indexes_in_desireddic}\")\n",
    "invalid_indexes_in_originaldata = [index for index in list(df_plot.index) if index not in list(label_dic.keys())]\n",
    "print(f\"WARNING ! These indexes in the dataframe where missed from your label_dic. Add a corresponding label to label_dic \\n {invalid_indexes_in_originaldata}\")\n",
    "\n",
    "#renmae models and reorder them\n",
    "df_plot.rename(index=label_dic, inplace=True)\n",
    "new_order = list(label_dic.values())  \n",
    "df_plot = df_plot.reindex(new_order)\n",
    "\n",
    "\n",
    "\n",
    "# Plotting the stacked bar chart\n",
    "bottom = None\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "bottom = None\n",
    "for key in color_map:\n",
    "    values = df_plot[key]\n",
    "    plt.bar(df_plot.index, values, bottom=bottom, color=color_map[key], label=key)\n",
    "    if key == 'Correct':\n",
    "        for i, value in enumerate(values):\n",
    "            if value>0:\n",
    "                plt.text(i, value -12, f\"{value:.1f}%\", ha='center', va='bottom',\n",
    "                        fontdict={'fontname': 'Arial', 'fontsize': 8}, color='black',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.3',alpha=0.5))\n",
    "                \n",
    "            # if i in [1]:\n",
    "            #     plt.text(i-0.25, 103,\"Web Environment\", va='bottom',\n",
    "            #              fontdict={'fontname': 'Arial', 'fontsize': 14}, color='black',\n",
    "            #              bbox=dict(facecolor='grey', edgecolor='none', boxstyle='round,pad=0.3',alpha=0.5)\n",
    "            #              )\n",
    "            # elif i in [9]:\n",
    "            #     plt.text(i-0.5, 103,\"Poe Environment\", va='bottom',\n",
    "            #              fontdict={'fontname': 'Arial', 'fontsize': 14}, color='black',\n",
    "            #              bbox=dict(facecolor='grey', edgecolor='none', boxstyle='round,pad=0.3',alpha=0.5)\n",
    "            #              )\n",
    "            # elif i in [15]:\n",
    "            #     plt.text(i, 103,\"API Environment\", va='bottom',\n",
    "            #              fontdict={'fontname': 'Arial', 'fontsize': 14}, color='black',\n",
    "            #              bbox=dict(facecolor='grey', edgecolor='none', boxstyle='round,pad=0.3',alpha=0.5)\n",
    "            #              )\n",
    "            # elif i in [22]:\n",
    "            #     plt.text(i+0.25, 103,\"Local Environment\", va='bottom',\n",
    "            #              fontdict={'fontname': 'Arial', 'fontsize': 14}, color='black',\n",
    "            #              bbox=dict(facecolor='grey', edgecolor='none', boxstyle='round,pad=0.3',alpha=0.5)\n",
    "            #              )\n",
    "    if bottom is None:\n",
    "        bottom = values\n",
    "    else:\n",
    "        bottom = bottom + values\n",
    "\n",
    "# Customizing the plot\n",
    "\n",
    "#plt.xlabel('Models')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title(f'All 2022 ACG questions (N = 300)', fontdict={'fontname': 'Arial', 'fontsize': 15}, )\n",
    "plt.legend()\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right', size=10)\n",
    "# Place legend outside the plot area\n",
    "plt.legend(loc='upper right',)\n",
    "\n",
    "# Color\n",
    "plt.grid(True, axis='y', color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "plt.gca().set_facecolor('lightgray')  # Set background color to gray\n",
    "# Remove top and right spines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"Submit\\Figures\\E1_Performance.jpg\", dpi=400)\n",
    "plt.savefig(r\"Submit\\Figures\\E1_Performance.png\", dpi=400)\n",
    "plt.savefig(r\"Submit\\Figures\\E1_Performance.pdf\", dpi=400)\n",
    "plt.savefig(r\"Submit\\Figures\\E1_Performance_imagevstext-total.jpg\", dpi=400)\n",
    "plt.savefig(r\"Submit\\Figures\\E1_Performance_imagevstext-total.png\", dpi=400)\n",
    "plt.savefig(r\"Submit\\Figures\\E1_Performance_imagevstext-total.pdf\", dpi=400)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E1- horizental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def interactive_count_values(df, columns, replacement_dictionary=None):\n",
    "    counts_dict = {}\n",
    "    replacement_dict = replacement_dictionary or {}\n",
    "    \n",
    "    for col in columns:\n",
    "        counts = df[col].value_counts(normalize=True) * 100\n",
    "        for value, percentage in counts.items():\n",
    "            if value not in replacement_dict:\n",
    "                replacement = input(f\"Enter replacement for '{value}': \")\n",
    "                replacement_dict[value] = replacement\n",
    "            \n",
    "            if replacement_dict[value] != \"\":\n",
    "                df[col] = df[col].replace({value: replacement_dict[value]})\n",
    "        \n",
    "        counts_dict[col] = df[col].value_counts(normalize=True) * 100\n",
    "    \n",
    "    return counts_dict, replacement_dict\n",
    "\n",
    "def draw_line(ax, pos, length, text):\n",
    "    line_x = data.max() + 10\n",
    "    ax.annotate(\n",
    "        text, \n",
    "        xy=(line_x + 5, pos), \n",
    "        xytext=(0, 0), textcoords=\"offset points\",\n",
    "        ha='left', va='center'\n",
    "    )\n",
    "    ax.plot([line_x, line_x + length], [pos, pos], color='black')\n",
    "\n",
    "# Load data from Excel file (replace 'excel_file_path' with your file path)\n",
    "excel_file_path = excel_file_path\n",
    "df = pd.read_excel(excel_file_path)\n",
    "df = df.apply(lambda x: re.sub('^Resolved: ', '', x) if isinstance(x, str) else x)\n",
    "\n",
    "correctness_columns = [col for col in df.columns if col.endswith('_correctness')]\n",
    "\n",
    "replacement_dictionary = replacement_dictionary\n",
    "\n",
    "model_correctness_counts, replacements = interactive_count_values(df, correctness_columns, replacement_dictionary=replacement_dictionary)\n",
    "\n",
    "print(\"\\nReplacement dictionary:\")\n",
    "print(replacements)\n",
    "\n",
    "df_plot = pd.DataFrame(model_correctness_counts).T.fillna(0)\n",
    "\n",
    "color_map = color_map\n",
    "desired_columns = [key for key in color_map]\n",
    "valid_columns = [col for col in desired_columns if col in df_plot.columns]\n",
    "missing_columns = [col for col in desired_columns if col not in df_plot.columns]\n",
    "for col in missing_columns:\n",
    "    df_plot[col] = 0\n",
    "df_plot = df_plot[desired_columns]\n",
    "\n",
    "label_dic = label_dic\n",
    "\n",
    "invalid_indexes_in_desireddic = [key for key, value in label_dic.items() if key not in df_plot.index]\n",
    "print(f\"WARNING ! These labels you have in your label_dic are not valid in the df.index Edit the label or remove it if you added this by mistake\\n {invalid_indexes_in_desireddic}\")\n",
    "invalid_indexes_in_originaldata = [index for index in list(df_plot.index) if index not in list(label_dic.keys())]\n",
    "print(f\"WARNING ! These indexes in the dataframe where missed from your label_dic. Add a corresponding label to label_dic \\n {invalid_indexes_in_originaldata}\")\n",
    "\n",
    "df_plot.rename(index=label_dic, inplace=True)\n",
    "new_order = list(label_dic.values())  \n",
    "new_order_reversed = new_order[::-1]  # Reverse the order\n",
    "df_plot = df_plot.reindex(new_order_reversed)\n",
    "\n",
    "plt.figure(figsize=(8.27, 11.69))\n",
    "\n",
    "left = None\n",
    "for key in color_map:\n",
    "    values = df_plot[key]\n",
    "    plt.barh(df_plot.index, values, left=left, color=color_map[key], label=key)\n",
    "    if key == 'Correct':\n",
    "        for i, value in enumerate(values):\n",
    "            if value > 0:\n",
    "                plt.text(value -10, i, f\"{value:.1f}%\", ha='left', va='center',\n",
    "                        fontdict={'fontname': 'Arial', 'fontsize': 8}, color='black',\n",
    "                        bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.3', alpha=0.5))\n",
    "    if left is None:\n",
    "        left = values\n",
    "    else:\n",
    "        left = left + values\n",
    "\n",
    "plt.xlabel('Percentage')\n",
    "plt.title(f'All 2022 ACG questions (N = 300)', fontdict={'fontname': 'Arial', 'fontsize': 15})\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.yticks(size=10)\n",
    "plt.grid(True, axis='x', color='white', linestyle='-', linewidth=0.5)\n",
    "plt.gca().set_facecolor('lightgray')\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Final Figures\\E1_Performance_horizontal.jpg\", dpi=400)\n",
    "plt.savefig(r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Final Figures\\E1_Performance_horizontal.png\", dpi=400)\n",
    "plt.savefig(r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Final Figures\\E1_Performance_horizontal.pdf\", dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STRATIFIED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E1-stratified-ImageText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data from Excel file (replace 'excel_file_path' with your file path)\n",
    "excel_file_path = excel_file_path\n",
    "df = pd.read_excel(excel_file_path)\n",
    "df = df.apply(lambda x: re.sub('^Resolved: ', '', x) if isinstance(x, str) else x)\n",
    "\n",
    "# List of unique question types\n",
    "question_types = df['Question Type'].unique()\n",
    "\n",
    "\n",
    "# Loop through each question type\n",
    "for question_type in question_types:\n",
    "    # Filter the DataFrame for the current question type\n",
    "    df_filtered = df[df['Question Type'] == question_type]\n",
    "    \n",
    "    # Calculate correctness columns for the filtered DataFrame\n",
    "    correctness_columns = [col for col in df.columns if col.endswith('_correctness')]\n",
    "    \n",
    "    # Dictionary to store counts of unique values for each model\n",
    "    replacement_dictionary=replacement_dictionary\n",
    "\n",
    "    # Call the function to interactively count values and handle replacements\n",
    "    model_correctness_counts, replacements = interactive_count_values(df_filtered, correctness_columns, replacement_dictionary=replacement_dictionary)\n",
    "\n",
    "    \n",
    "    # Transform the dictionary into a DataFrame\n",
    "    df_plot = pd.DataFrame(model_correctness_counts).T.fillna(0)\n",
    "    \n",
    "    # labeling -------------------------------\n",
    "    \n",
    "    #color_map was previously\n",
    "    color_map = color_map\n",
    "    # Reordering columns to match color map\n",
    "    desired_columns = [key for key in color_map]\n",
    "    valid_columns = [col for col in desired_columns if col in df_plot.columns]\n",
    "    missing_columns = [col for col in desired_columns if col not in df_plot.columns]\n",
    "    for col in missing_columns:\n",
    "        df_plot[col] = 0\n",
    "    df_plot = df_plot[desired_columns]\n",
    "\n",
    "\n",
    "    # labeling and sorting indexes (models) -------------------------------\n",
    "    label_dic=label_dic\n",
    "\n",
    "    #invalid_indexes_in_desireddic = [key for key, value in label_dic.items() if key not in df_plot.index]\n",
    "    #print(f\"WARNING ! These labels you have in your label_dic are not valid in the df.index Edit the label or remove it if you added this by mistake\\n {invalid_indexes_in_desireddic}\")\n",
    "    #invalid_indexes_in_originaldata = [index for index in list(df_plot.index) if index not in list(label_dic.keys())]\n",
    "    #print(f\"WARNING ! These indexes in the dataframe where missed from your label_dic. Add a corresponding label to label_dic \\n {invalid_indexes_in_originaldata}\")\n",
    "\n",
    "    #renmae models and reorder them\n",
    "    df_plot.rename(index=label_dic, inplace=True)\n",
    "    new_order = list(label_dic.values())  \n",
    "    df_plot = df_plot.reindex(new_order)\n",
    "\n",
    "\n",
    "\n",
    "    # Plotting the stacked bar chart\n",
    "    bottom = None\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    bottom = None\n",
    "    for key in color_map:\n",
    "        values = df_plot[key]\n",
    "        plt.bar(df_plot.index, values, bottom=bottom, color=color_map[key], label=key)\n",
    "        if key == 'Correct':\n",
    "            for i, value in enumerate(values):\n",
    "                if value>0:\n",
    "                    plt.text(i, value -12, f\"{value:.1f}%\", ha='center', va='bottom',\n",
    "                            fontdict={'fontname': 'Arial', 'fontsize': 8}, color='black',\n",
    "                            bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.3',alpha=0.5))\n",
    "                    \n",
    "        if bottom is None:\n",
    "            bottom = values\n",
    "        else:\n",
    "            bottom = bottom + values\n",
    "\n",
    "    # Customizing the plot\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title(f'Question type: {question_type} (N = {df_filtered.shape[0]})',  fontdict={'fontname': 'Arial', 'fontsize': 15}, )\n",
    "    plt.legend()\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=55, ha='right')\n",
    "    # Place legend outside the plot area\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    # Color\n",
    "    plt.grid(True, axis='y', color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "    plt.gca().set_facecolor('lightgray')  # Set background color to gray\n",
    "    # Remove top and right spines\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_imagevstext-{question_type}.jpg\", dpi=400,bbox_inches='tight')\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_imagevstext-{question_type}.png\", dpi=400,bbox_inches='tight')\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_imagevstext-{question_type}.pdf\", dpi=400,bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# stacked image\n",
    "directory = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figures\"\n",
    "prefix = r'E1_Performance_imagevstext-'\n",
    "num_columns=1\n",
    "\n",
    "give_list_instead=[\n",
    "    r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figures\\E1_Performance_imagevstext-total.png\",\n",
    "    r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figures\\E1_Performance_imagevstext-text-based.png\",\n",
    "    r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figures\\E1_Performance_imagevstext-image-inclusive.png\"\n",
    "]\n",
    "add_subplot_tags_and_stack(directory, prefix,num_columns,give_list_instead=give_list_instead)\n",
    "\n",
    "#import shutil\n",
    "#Final_save_dic={\n",
    "#    r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Final Figures\\E1_Performance_imagevstext-__stacked_image.png\":\n",
    "#        r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figure1.png\",\n",
    "#    r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Final Figures\\E1_Performance_imagevstext-__stacked_image.pdf\":\n",
    "#        r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figure1.pdf\",\n",
    "#}\n",
    "#for key, value in Final_save_dic.items():\n",
    "#    shutil.copy(src=key,dst=value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E1-stratified-subcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startified_colomn_name='Qtype Category'\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data from Excel file (replace 'excel_file_path' with your file path)\n",
    "excel_file_path = excel_file_path\n",
    "df = pd.read_excel(excel_file_path)\n",
    "df = df.apply(lambda x: re.sub('^Resolved: ', '', x) if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# List of unique question types\n",
    "question_types = df[startified_colomn_name].unique()\n",
    "\n",
    "\n",
    "# Loop through each question type\n",
    "for question_type in question_types:\n",
    "    # Filter the DataFrame for the current question type\n",
    "    df_filtered = df[df[startified_colomn_name] == question_type]\n",
    "    \n",
    "    # Calculate correctness columns for the filtered DataFrame\n",
    "    correctness_columns = [col for col in df.columns if col.endswith('_correctness')]\n",
    "    \n",
    "    # Dictionary to store counts of unique values for each model\n",
    "    replacement_dictionary=replacement_dictionary\n",
    "    # Call the function to interactively count values and handle replacements\n",
    "    model_correctness_counts, replacements = interactive_count_values(df_filtered, correctness_columns, replacement_dictionary=replacement_dictionary)\n",
    "\n",
    "    \n",
    "    # Transform the dictionary into a DataFrame\n",
    "    df_plot = pd.DataFrame(model_correctness_counts).T.fillna(0)\n",
    "    \n",
    "    # labeling -------------------------------\n",
    "    \n",
    "    #color_map was previously\n",
    "    color_map = color_map\n",
    "    # Reordering columns to match color map\n",
    "    desired_columns = [key for key in color_map]\n",
    "    valid_columns = [col for col in desired_columns if col in df_plot.columns]\n",
    "    missing_columns = [col for col in desired_columns if col not in df_plot.columns]\n",
    "    for col in missing_columns:\n",
    "        df_plot[col] = 0\n",
    "    df_plot = df_plot[desired_columns]\n",
    "\n",
    "\n",
    "    # labeling and sorting indexes (models) -------------------------------\n",
    "    label_dic=label_dic\n",
    "\n",
    "    invalid_indexes_in_desireddic = [key for key, value in label_dic.items() if key not in df_plot.index]\n",
    "    print(f\"WARNING ! These labels you have in your label_dic are not valid in the df.index Edit the label or remove it if you added this by mistake\\n {invalid_indexes_in_desireddic}\")\n",
    "    invalid_indexes_in_originaldata = [index for index in list(df_plot.index) if index not in list(label_dic.keys())]\n",
    "    print(f\"WARNING ! These indexes in the dataframe where missed from your label_dic. Add a corresponding label to label_dic \\n {invalid_indexes_in_originaldata}\")\n",
    "\n",
    "    #renmae models and reorder them\n",
    "    df_plot.rename(index=label_dic, inplace=True)\n",
    "    new_order = list(label_dic.values())  \n",
    "    df_plot = df_plot.reindex(new_order)\n",
    "\n",
    "\n",
    "\n",
    "    # Plotting the stacked bar chart\n",
    "    bottom = None\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    bottom = None\n",
    "    for key in color_map:\n",
    "        values = df_plot[key]\n",
    "        plt.bar(df_plot.index, values, bottom=bottom, color=color_map[key], label=key)\n",
    "        if key == 'Correct':\n",
    "            for i, value in enumerate(values):\n",
    "                if value>0:\n",
    "                    plt.text(i, value -12, f\"{value:.1f}%\", ha='center', va='bottom',\n",
    "                            fontdict={'fontname': 'Arial', 'fontsize': 8}, color='black',\n",
    "                            bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.3',alpha=0.5))\n",
    "\n",
    "        if bottom is None:\n",
    "            bottom = values\n",
    "        else:\n",
    "            bottom = bottom + values\n",
    "\n",
    "    # Customizing the plot\n",
    "    #plt.xlabel('Models')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title(f'Question type: {question_type} (N = {df_filtered.shape[0]})', fontdict={'fontname': 'Arial', 'fontsize': 15}, )\n",
    "    plt.legend()\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=55, ha='right')\n",
    "    # Place legend outside the plot area\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    # Color\n",
    "    plt.grid(True, axis='y', color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "    plt.gca().set_facecolor('lightgray')  # Set background color to gray\n",
    "    # Remove top and right spines\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_subcategory-{question_type}.jpg\", dpi=400, bbox_inches='tight')\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_subcategory-{question_type}.png\", dpi=400, bbox_inches='tight')\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_subcategory-{question_type}.pdf\", dpi=400, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# stacked image\n",
    "directory = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figures\"\n",
    "prefix = 'E1_Performance_subcategory-'\n",
    "num_columns=2\n",
    "add_subplot_tags_and_stack(directory, prefix,num_columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the extra pancreato biliary\n",
    "directory = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figures\"\n",
    "prefix = 'E1_Performance_subcategory-'\n",
    "num_columns=2\n",
    "add_subplot_tags_and_stack(directory, prefix,num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E1-stratified-Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "startified_colomn_name='Qtype Length'\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data from Excel file (replace 'excel_file_path' with your file path)\n",
    "excel_file_path = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\__Milestone Datasets\\E1_Bestprompt-2022_all_Final_E2_manualevaluatedcompleted_Final_E2_Final.xlsx\"\n",
    "df = pd.read_excel(excel_file_path)\n",
    "df = df.apply(lambda x: re.sub('^Resolved: ', '', x) if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# List of unique question types\n",
    "question_types = df[startified_colomn_name].unique()\n",
    "\n",
    "\n",
    "# Loop through each question type\n",
    "for question_type in question_types:\n",
    "    # Filter the DataFrame for the current question type\n",
    "    df_filtered = df[df[startified_colomn_name] == question_type]\n",
    "    \n",
    "    # Calculate correctness columns for the filtered DataFrame\n",
    "    correctness_columns = [col for col in df.columns if col.endswith('_correctness')]\n",
    "    \n",
    "    # Dictionary to store counts of unique values for each model\n",
    "    replacement_dictionary=replacement_dictionary\n",
    "\n",
    "    # Call the function to interactively count values and handle replacements\n",
    "    model_correctness_counts, replacements = interactive_count_values(df_filtered, correctness_columns, replacement_dictionary=replacement_dictionary)\n",
    "\n",
    "    \n",
    "    # Transform the dictionary into a DataFrame\n",
    "    df_plot = pd.DataFrame(model_correctness_counts).T.fillna(0)\n",
    "    \n",
    "    # labeling -------------------------------\n",
    "    \n",
    "    #color_map was previously\n",
    "    color_map = color_map\n",
    "    # Reordering columns to match color map\n",
    "    desired_columns = [key for key in color_map]\n",
    "    valid_columns = [col for col in desired_columns if col in df_plot.columns]\n",
    "    missing_columns = [col for col in desired_columns if col not in df_plot.columns]\n",
    "    for col in missing_columns:\n",
    "        df_plot[col] = 0\n",
    "    df_plot = df_plot[desired_columns]\n",
    "\n",
    "\n",
    "    # labeling and sorting indexes (models) -------------------------------\n",
    "    # labeling and sorting indexes (models) -------------------------------\n",
    "    # label_dic=label_dic\n",
    "\n",
    "    # invalid_indexes_in_desireddic = [key for key, value in label_dic.items() if key not in df_plot.index]\n",
    "    # print(f\"WARNING ! These labels you have in your label_dic are not valid in the df.index Edit the label or remove it if you added this by mistake\\n {invalid_indexes_in_desireddic}\")\n",
    "    # invalid_indexes_in_originaldata = [index for index in list(df_plot.index) if index not in list(label_dic.keys())]\n",
    "    # print(f\"WARNING ! These indexes in the dataframe where missed from your label_dic. Add a corresponding label to label_dic \\n {invalid_indexes_in_originaldata}\")\n",
    "\n",
    "    #renmae models and reorder them\n",
    "    df_plot.rename(index=label_dic, inplace=True)\n",
    "    new_order = list(label_dic.values())  \n",
    "    df_plot = df_plot.reindex(new_order)\n",
    "\n",
    "\n",
    "\n",
    "    # Plotting the stacked bar chart\n",
    "    bottom = None\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    bottom = None\n",
    "    for key in color_map:\n",
    "        values = df_plot[key]\n",
    "        plt.bar(df_plot.index, values, bottom=bottom, color=color_map[key], label=key)\n",
    "        if key == 'Correct':\n",
    "            for i, value in enumerate(values):\n",
    "                if value>0:\n",
    "                    plt.text(i, value -12, f\"{value:.1f}%\", ha='center', va='bottom',\n",
    "                            fontdict={'fontname': 'Arial', 'fontsize': 8}, color='black',\n",
    "                            bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.3',alpha=0.5))\n",
    "\n",
    "        if bottom is None:\n",
    "            bottom = values\n",
    "        else:\n",
    "            bottom = bottom + values\n",
    "\n",
    "    # Customizing the plot\n",
    "    #plt.xlabel('Models')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title(f'Question type: {question_type} (N = {df_filtered.shape[0]})', fontdict={'fontname': 'Arial', 'fontsize': 15}, )\n",
    "    plt.legend()\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=55, ha='right')\n",
    "    # Place legend outside the plot area\n",
    "    plt.legend(loc='upper right', )\n",
    "    \n",
    "    # Color\n",
    "    plt.grid(True, axis='y', color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "    plt.gca().set_facecolor('lightgray')  # Set background color to gray\n",
    "    # Remove top and right spines\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    # Show plot\n",
    "    #plt.tight_layout()\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_Qlength-{question_type}.jpg\", dpi=400, bbox_inches='tight')\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_Qlength-{question_type}.png\", dpi=400, bbox_inches='tight')\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_Qlength-{question_type}.pdf\", dpi=400, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked image\n",
    "directory = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figures\"\n",
    "prefix = 'E1_Performance_Qlength-'\n",
    "num_columns=1\n",
    "add_subplot_tags_and_stack(directory, prefix,num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E1-stratified-Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "startified_colomn_name='Qtype Difficulty'\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data from Excel file (replace 'excel_file_path' with your file path)\n",
    "excel_file_path = excel_file_path\n",
    "df = pd.read_excel(excel_file_path)\n",
    "df = df.apply(lambda x: re.sub('^Resolved: ', '', x) if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# List of unique question types\n",
    "question_types = df[startified_colomn_name].unique()\n",
    "\n",
    "\n",
    "# Loop through each question type\n",
    "for question_type in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "    # Filter the DataFrame for the current question type\n",
    "    df_filtered = df[df[startified_colomn_name] == question_type]\n",
    "    \n",
    "    # Calculate correctness columns for the filtered DataFrame\n",
    "    correctness_columns = [col for col in df.columns if col.endswith('_correctness')]\n",
    "    \n",
    "    # Dictionary to store counts of unique values for each model\n",
    "    replacement_dictionary= replacement_dictionary\n",
    "\n",
    "    # Call the function to interactively count values and handle replacements\n",
    "    model_correctness_counts, replacements = interactive_count_values(df_filtered, correctness_columns, replacement_dictionary=replacement_dictionary)\n",
    "\n",
    "    \n",
    "    # Transform the dictionary into a DataFrame\n",
    "    df_plot = pd.DataFrame(model_correctness_counts).T.fillna(0)\n",
    "    \n",
    "    # labeling -------------------------------\n",
    "    \n",
    "    #color_map was previously\n",
    "    color_map = color_map\n",
    "    # Reordering columns to match color map\n",
    "    desired_columns = [key for key in color_map]\n",
    "    valid_columns = [col for col in desired_columns if col in df_plot.columns]\n",
    "    missing_columns = [col for col in desired_columns if col not in df_plot.columns]\n",
    "    for col in missing_columns:\n",
    "        df_plot[col] = 0\n",
    "    df_plot = df_plot[desired_columns]\n",
    "\n",
    "\n",
    "    # labeling and sorting indexes (models) -------------------------------\n",
    "    label_dic=label_dic\n",
    "\n",
    "    invalid_indexes_in_desireddic = [key for key, value in label_dic.items() if key not in df_plot.index]\n",
    "    print(f\"WARNING ! These labels you have in your label_dic are not valid in the df.index Edit the label or remove it if you added this by mistake\\n {invalid_indexes_in_desireddic}\")\n",
    "    invalid_indexes_in_originaldata = [index for index in list(df_plot.index) if index not in list(label_dic.keys())]\n",
    "    print(f\"WARNING ! These indexes in the dataframe where missed from your label_dic. Add a corresponding label to label_dic \\n {invalid_indexes_in_originaldata}\")\n",
    "\n",
    "    #renmae models and reorder them\n",
    "    df_plot.rename(index=label_dic, inplace=True)\n",
    "    new_order = list(label_dic.values())  \n",
    "    df_plot = df_plot.reindex(new_order)\n",
    "\n",
    "\n",
    "\n",
    "    # Plotting the stacked bar chart\n",
    "    bottom = None\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    bottom = None\n",
    "    for key in color_map:\n",
    "        values = df_plot[key]\n",
    "        plt.bar(df_plot.index, values, bottom=bottom, color=color_map[key], label=key)\n",
    "        if key == 'Correct':\n",
    "            for i, value in enumerate(values):\n",
    "                if value>0:\n",
    "                    plt.text(i, value -12, f\"{value:.1f}%\", ha='center', va='bottom',\n",
    "                            fontdict={'fontname': 'Arial', 'fontsize': 8}, color='black',\n",
    "                            bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.3',alpha=0.5))\n",
    "                    \n",
    "        if bottom is None:\n",
    "            bottom = values\n",
    "        else:\n",
    "            bottom = bottom + values\n",
    "\n",
    "    # Customizing the plot\n",
    "    #plt.xlabel('Models')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title(f'Question Difficulty (Average Human Score): {question_type} (N = {df_filtered.shape[0]})',  fontdict={'fontname': 'Arial', 'fontsize': 15}, )\n",
    "    plt.legend()\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=55, ha='right')\n",
    "    # Place legend outside the plot area\n",
    "    plt.legend(loc='upper right',)\n",
    "    \n",
    "    # Color\n",
    "    plt.grid(True, axis='y', color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "    plt.gca().set_facecolor('lightgray')  # Set background color to gray\n",
    "    # Remove top and right spines\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    # Show plot\n",
    "    #plt.tight_layout()\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_Qdifficulty-{question_type}.jpg\", dpi=400, bbox_inches='tight')\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_Qdifficulty-{question_type}.png\", dpi=400, bbox_inches='tight')\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_Qdifficulty-{question_type}.pdf\", dpi=400, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked image\n",
    "directory = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figures\"\n",
    "prefix = 'E1_Performance_Qdifficulty-'\n",
    "num_columns=1\n",
    "add_subplot_tags_and_stack(directory, prefix,num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E1-startified- Patient care phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "patient_care_dict={\n",
    "    \"Diagnosis\" : df[df[\"Qtype Care Phase - Diagsis\"] == \"Yes\"],\n",
    "    \"Treatment\" : df[df[\"Qtype Care Phase - Treatment\"] == \"Yes\"],\n",
    "    \"Complication\" : df[df[\"Qtype Care Phase - Complications\"] == \"Yes\"],\n",
    "    \"Investigation\" : df[df[\"Qtype Care Phase - Medical Investigation\"] == \"Yes\"],\n",
    "    \"Pathophisiology\" : df[df[\"Qtype Care Phase - Pathophyisiology\"] == \"Yes\"],\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data from Excel file (replace 'excel_file_path' with your file path)\n",
    "excel_file_path = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\__Milestone Datasets\\E1_Bestprompt-2022_all_Final_E2_manualevaluatedcompleted_Final_E2_Final.xlsx\"\n",
    "df = pd.read_excel(excel_file_path)\n",
    "df = df.apply(lambda x: re.sub('^Resolved: ', '', x) if isinstance(x, str) else x)\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each question type\n",
    "for question_type, df_filtered in patient_care_dict.items():\n",
    "\n",
    "    \n",
    "    # Calculate correctness columns for the filtered DataFrame\n",
    "    correctness_columns = [col for col in df.columns if col.endswith('_correctness')]\n",
    "    \n",
    "    # Dictionary to store counts of unique values for each model\n",
    "    replacement_dictionary=replacement_dictionary\n",
    "\n",
    "    # Call the function to interactively count values and handle replacements\n",
    "    model_correctness_counts, replacements = interactive_count_values(df_filtered, correctness_columns, replacement_dictionary=replacement_dictionary)\n",
    "\n",
    "    \n",
    "    # Transform the dictionary into a DataFrame\n",
    "    df_plot = pd.DataFrame(model_correctness_counts).T.fillna(0)\n",
    "    \n",
    "    # labeling -------------------------------\n",
    "    \n",
    "    #color_map was previously\n",
    "    color_map = color_map\n",
    "    # Reordering columns to match color map\n",
    "    desired_columns = [key for key in color_map]\n",
    "    valid_columns = [col for col in desired_columns if col in df_plot.columns]\n",
    "    missing_columns = [col for col in desired_columns if col not in df_plot.columns]\n",
    "    for col in missing_columns:\n",
    "        df_plot[col] = 0\n",
    "    df_plot = df_plot[desired_columns]\n",
    "\n",
    "\n",
    "    # labeling and sorting indexes (models) -------------------------------\n",
    "    label_dic=label_dic\n",
    "\n",
    "    invalid_indexes_in_desireddic = [key for key, value in label_dic.items() if key not in df_plot.index]\n",
    "    print(f\"WARNING ! These labels you have in your label_dic are not valid in the df.index Edit the label or remove it if you added this by mistake\\n {invalid_indexes_in_desireddic}\")\n",
    "    invalid_indexes_in_originaldata = [index for index in list(df_plot.index) if index not in list(label_dic.keys())]\n",
    "    print(f\"WARNING ! These indexes in the dataframe where missed from your label_dic. Add a corresponding label to label_dic \\n {invalid_indexes_in_originaldata}\")\n",
    "\n",
    "    #renmae models and reorder them\n",
    "    df_plot.rename(index=label_dic, inplace=True)\n",
    "    new_order = list(label_dic.values())  \n",
    "    df_plot = df_plot.reindex(new_order)\n",
    "\n",
    "\n",
    "\n",
    "    # Plotting the stacked bar chart\n",
    "    bottom = None\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    bottom = None\n",
    "    for key in color_map:\n",
    "        values = df_plot[key]\n",
    "        plt.bar(df_plot.index, values, bottom=bottom, color=color_map[key], label=key)\n",
    "        if key == 'Correct':\n",
    "            for i, value in enumerate(values):\n",
    "                if value>0:\n",
    "                    plt.text(i, value -12, f\"{value:.1f}%\", ha='center', va='bottom',\n",
    "                            fontdict={'fontname': 'Arial', 'fontsize': 8}, color='black',\n",
    "                            bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.3',alpha=0.5))\n",
    "\n",
    "        if bottom is None:\n",
    "            bottom = values\n",
    "        else:\n",
    "            bottom = bottom + values\n",
    "\n",
    "    # Customizing the plot\n",
    "    #plt.xlabel('Models')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title(f'Question type: {question_type} (N = {df_filtered.shape[0]})', fontdict={'fontname': 'Arial', 'fontsize': 15}, )\n",
    "    plt.legend()\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=55, ha='right')\n",
    "    # Place legend outside the plot area\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    # Color\n",
    "    plt.grid(True, axis='y', color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "    plt.gca().set_facecolor('lightgray')  # Set background color to gray\n",
    "    # Remove top and right spines\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    # Show plot\n",
    "    #plt.tight_layout()\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_Ptphase-{question_type}.jpg\", dpi=400, bbox_inches='tight')\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_Ptphase-{question_type}.png\", dpi=400, bbox_inches='tight')\n",
    "    plt.savefig(f\"Submit\\\\Figures\\\\E1_Performance_Ptphase-{question_type}.pdf\", dpi=400, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# stacked image\n",
    "directory = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figures\"\n",
    "prefix = 'E1_Performance_Ptphase-'\n",
    "\n",
    "num_columns=1\n",
    "add_subplot_tags_and_stack(directory, prefix,num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E1 heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dic={\n",
    "    # Web\n",
    "        #GPTs\n",
    "    'GPT3.5Web_correctness':\"GPT-3.5 Web\",\n",
    "    'gpt-3.5-turbo-0125_correctness': \"GPT-3.5 API\",\n",
    "    'GPT4Web_correctness':\"GPT-4 Web\", \n",
    "    'gpt-4-0613_correctness':\"GPT-4 API\",\n",
    "    'gpt-4o-2024-05-13_correctness': \"GPT-4o API\",\n",
    "    'gpt-4o-mini-2024-07-18_correctness':\"GPT-4omini API\",           \n",
    "    r\" \": r\" \",        \n",
    "            \n",
    "            \n",
    "    #Claude\n",
    "    'ClaudeHiakuWeb-raw_correctness':\"Claude3-Hiaku Web\",\n",
    "    'claude-3-sonnet-20240229_correctness':\"Claude3-Haiku API\",  \n",
    "    'ClaudeSonnetWeb-raw_correctness': \"Claude3-Sonnet Web\",\n",
    "    'claude-3-haiku-20240307_correctness':\"Claude3-Sonnet API\", \n",
    "    'ClaudeOpuWeb-raw_correctness':\"Claude3-Opus Web\", \n",
    "    'claude-3-opus-20240229_correctness':\"Claude3-Opus API\",\n",
    "    'claude-3-5-sonnet-20240620_correctness': \"Claude3.5-Sonnet API\",\n",
    "    r\"  \": r\"  \",    \n",
    "        \n",
    "    \n",
    "    # Poe\n",
    "    'Mistral-7B-T_correctness':\"Mistralv2-7b Poe\",\n",
    "    'mistral-instruct-v2-Q8_correctness':\"Mistralv2-7b-Q8 Local\",\n",
    "    'MistralLarge-Poe_correctness':\"Mistral-Large Poe\",\n",
    "    'Mixtral-8x7B-Poe_correctness':\"Mixtral-8x7b Poe\",\n",
    "    \n",
    "    r\"     \": r\"     \",\n",
    "    'Llama27B-Poe_correctness': \"Llama2-7b Poe\", \n",
    "    'llama2-7B-Q8_correctness':\"Llama2-7b-Q8 Local\",\n",
    "    'medicine-chat-Q8_correctness':\"medicineLLM-7b-Q8 Local\",\n",
    "    'Llama-2-13b_correctness': 'Llama2-13b Poe',\n",
    "    'llama2-13B-Q5KM_correctness':\"Llama2-13b-Q5 Local\", \n",
    "    'Llama270B-Poe_correctness':\"Llama2-70b Poe\",\n",
    "    'Llama3-8b-Poe_correctness':'Llama3-8b Poe',\n",
    "    \"llama3-8b-Q8_correctness\": \"Llama3-8b-Q8 Local\", \n",
    "    'openbioLLM-7B-Q8_correctness': \"OpenBioLLM-8b-Q8 Local\", \n",
    "    'Llama3-70b-Poe_correctness':'Llama3-70b Poe',\n",
    "    \"Llama3.1-8B_correctness\": \"Llama3.1-8b Poe\",\n",
    "    \"Llama3.1-70B_correctness\": \"Llama3.1-70b Poe\",\n",
    "    \"Llama3.1-405B_correctness\": \"Llama3.1-405b Poe\",\n",
    "    \n",
    "    r\"       \": r\"       \",\n",
    "    \n",
    "        # Gemini\n",
    "    'GeminiWeb-raw_correctness':\"Gemini Web\",\n",
    "    'GeminiAdvancedWeb-raw_correctness':\"GeminiAdvanced Web\", \n",
    "    'gemma2-9b-it_correctness': \"Gemma2-9b Poe\",\n",
    "    'Gemma2-9b-Q8_correctness': \"Gemma2-9b-Q8 Local\",\n",
    "    \"gemma2-27b-it_correctness\": \"Gemma2-27b Poe\",\n",
    "    r\"    \": r\"    \",\n",
    "\n",
    "    'phi3-3b-Q16_correctness':'Phi3-3b-Q16 Local',\n",
    "    'Phi3-medium14b-Q6_correctness':'Phi3-14b-Q6 Local',\n",
    "\n",
    "    r\"                       \": r\"                   \",     \n",
    "    'Qtype Difficulty':'Difficulty Quartiles',\n",
    "    r\"            \": r\"           \",\n",
    "    'Qtype Length': 'Length Tertiles',\n",
    "    r\"      \": r\"      \",\n",
    "    'Qtype Taxonomy - Integrated': 'Taxonomy: Integrated',\n",
    "    r\"         \": r\"          \",\n",
    "    \n",
    "    'Qtype Care Phase - Diagsis': 'Diagnosis',\n",
    "    'Qtype Care Phase - Treatment': 'Treatment',\n",
    "    'Qtype Care Phase - Medical Investigation': 'Investigation',\n",
    "    'Qtype Care Phase - Complications': 'Complications',\n",
    "    r\"          \": r\"          \",\n",
    "    \n",
    "    'Category - Liver': 'Liver',\n",
    "    'Category - COLON': 'Colon',\n",
    "    'Category - Esophagus': 'Esophagus',\n",
    "    'Category - IBD': 'IBD',\n",
    "    'Category - Pancreatico-biliary': 'Pancreatico-biliary',\n",
    "    \n",
    "    }\n",
    "\n",
    "\n",
    "replacement_dictionary = {'Incorrect': 'Incorrect', 'incorrect': 'Incorrect', 'Correct': 'Correct', 'correct': 'Correct', 'EOP': 'EOP', '2OP': '2OP', 'NOP': 'NOP', 'No answer': 'Error', 'NoA': 'Error'}\n",
    "color_map= {\n",
    "        \"Correct\": \"#008000\",  # green\n",
    "        \"2OP\": \"#90EE90\",  # light green\n",
    "        \"EOP\": \"#D3D3D3\",  # light grey\n",
    "        \"NOP\": \"#A9A9A9\",  # grey\n",
    "        \"Error\": \"#ffa500\",\n",
    "        \"Incorrect\": \"#FF0000\",  # red\n",
    "        \n",
    "        \"Short\": \"#DDFFF9\",\n",
    "        \"Medium\": \"#9DE5FF\" ,\n",
    "        \"Long\": \"#647AFF\",\n",
    "            \n",
    "        'Q1': \"#FF7AF4\",\n",
    "        'Q2': \"#F58AFF\",\n",
    "        'Q3': \"#D5BDFF\",\n",
    "        'Q4': \"#E2E3FF\",\n",
    "        \n",
    "        'Yes': \"#6c757d\",\n",
    "        'No': \"#ffffff\",\n",
    "        \n",
    "    }\n",
    "\n",
    "excel_file_path = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\__Milestone Datasets\\E1_Bestprompt-2022_all_Final_E2_manualevaluatedcompleted_Final_E2_Final.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def rename_and_reorder_columns(excel_file, label_dic):\n",
    "    # Load the Excel file into a DataFrame\n",
    "    df = pd.read_excel(excel_file)\n",
    "    \n",
    "    # Rename the columns based on the dictionary\n",
    "    df.rename(columns=label_dic, inplace=True)\n",
    "    \n",
    "    # Add dummy columns for spacing in label_dic where needed\n",
    "    modified_columns = []\n",
    "    for key in label_dic:\n",
    "        if key.strip():  # Only add actual model names\n",
    "            modified_columns.append(label_dic[key])\n",
    "        else:\n",
    "            # Create a unique dummy name for empty spaces\n",
    "            dummy_name = ' ' * (modified_columns.count(' ') + 1)\n",
    "            df[dummy_name] = float('nan')  # Add NaN column for spacing\n",
    "            modified_columns.append(dummy_name)\n",
    "    \n",
    "    # Reorder the columns including dummy columns\n",
    "    df = df[modified_columns]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    # Remove 'Resolved: ' prefix from string entries in the DataFrame\n",
    "    df = df.apply(lambda x: re.sub('^Resolved: ', '', x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Strip whitespace from all string entries in the DataFrame\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def replace_values(df, replacement_dictionary):\n",
    "    # Replace specific values according to the replacement dictionary\n",
    "    df.replace(replacement_dictionary, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Call the function to rename and reorder columns\n",
    "new_df = rename_and_reorder_columns(excel_file_path, label_dic)\n",
    "\n",
    "# Clean data in the DataFrame\n",
    "cleaned_df = clean_data(new_df)\n",
    "\n",
    "# Replace values in the DataFrame\n",
    "final_df = replace_values(cleaned_df, replacement_dictionary)\n",
    "\n",
    "# Counting \"Correct\" in each row\n",
    "final_df['CorrectCount'] = final_df.apply(lambda row: (row == 'Correct').sum(), axis=1)\n",
    "\n",
    "# Sorting the dataframe based on the count of \"Correct\"\n",
    "final_df_sorted = final_df.sort_values(by='CorrectCount', ascending=False)\n",
    "\n",
    "# Optionally, you can drop the helper column if it's no longer needed\n",
    "final_df_sorted = final_df_sorted.drop(columns=['CorrectCount'])\n",
    "final_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def draw_custom_heatmap(df, color_map):\n",
    "    # Create a figure and a subplot\n",
    "    fig, ax = plt.subplots(figsize=(11, 15))  # Adjust size to fit the number of columns or use dynamic sizing\n",
    "    \n",
    "    # Number of models (rows) and metrics (columns)\n",
    "    num_models = len(df)\n",
    "    num_metrics = len(df.columns)\n",
    "    \n",
    "    # Set the ticks for x and y axes\n",
    "    ax.set_xticks(range(num_metrics))\n",
    "    ax.set_yticks(range(num_models))\n",
    "    ax.set_xticklabels(df.columns, rotation=90)\n",
    "    ax.set_yticklabels(df.index)\n",
    "    \n",
    "    # Draw each cell as a colored rectangle\n",
    "    for i, (idx, row) in enumerate(df.iterrows()):\n",
    "        for j, val in enumerate(row):\n",
    "            color = color_map.get(val, 'white')  # Default color is white if not specified in color_map\n",
    "            ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=True, color=color))\n",
    "\n",
    "    # Adjust axis limits to make sure all rows and columns are included\n",
    "    ax.set_xlim(0, num_metrics)\n",
    "    ax.set_ylim(0, num_models)\n",
    "\n",
    "    # Hide the grid and frame\n",
    "    ax.grid(color='white')\n",
    "    # Customize spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.xaxis.set_visible(False)\n",
    "\n",
    "    # This is to ensure the rows are displayed in the order as in the dataframe\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.savefig(\"C:\\\\Users\\\\LEGION\\\\Documents\\GIT\\\\LLM_answer_GIBoard\\Submit\\\\Final Figures\\\\heatmap.jpg\", dpi=400, bbox_inches='tight')\n",
    "    plt.savefig(\"C:\\\\Users\\\\LEGION\\\\Documents\\GIT\\\\LLM_answer_GIBoard\\Submit\\\\Final Figures\\\\heatmap.png\", dpi=400, bbox_inches='tight')\n",
    "    plt.savefig(\"C:\\\\Users\\\\LEGION\\\\Documents\\GIT\\\\LLM_answer_GIBoard\\Submit\\\\Final Figures\\\\heatmap.pdf\", dpi=400, bbox_inches='tight')\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Draw the heatmap\n",
    "draw_custom_heatmap(final_df_sorted.T, color_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dictionary = {'Incorrect': 'Incorrect', 'incorrect': 'Incorrect', 'Correct': 'Correct', 'correct': 'Correct', 'EOP': 'EOP', '2OP': '2OP', 'NOP': 'NOP', 'No answer': 'Error', 'NoA': 'Error'}\n",
    "color_map= {\n",
    "        \"Correct\": \"#008000\",  # green\n",
    "        \"2OP\": \"#90EE90\",  # light green\n",
    "        \"EOP\": \"#D3D3D3\",  # light grey\n",
    "        \"NOP\": \"#A9A9A9\",  # grey\n",
    "        \"Error\": \"#ffa500\",\n",
    "        \"Incorrect\": \"#FF0000\",  # red       \n",
    "    }\n",
    "\n",
    "# labeling and sorting indexes (models) -------------------------------\n",
    "label_dic_GPT4Web={\n",
    "    'GPT4Web_correctness':\"GPT4 Web - no image\", \n",
    "    'GPT4Web_GPT4VWebCapQ_correctness':'GPT4 Web - LLM caption',\n",
    "    'GPT4VWeb_Direct_correctness':'GPT4 Web - direct image',\n",
    "    'GPT4Web_HumanCapQ_correctness':'GPT4 Web - human hint',}\n",
    "\n",
    "label_dic_ClaudeOpusWeb={\n",
    "    'ClaudeOpuWeb-raw_correctness':\"Claude3Opus Web - no image\",\n",
    "    'Claude3OpusWeb_Claude3OpusWebCapQ_correctness': \"Claude3Opus Web - LLM caption\",\n",
    "    'Claude3OpusVWeb_Direct_correctness':  \"Claude3Opus Web - direct image\",\n",
    "    'Claude3OpusWeb_HumanCapQ_correctness': \"Claude3Opus Web - human hint\",\n",
    "    }\n",
    "\n",
    "label_dic_GeminiAdvWeb={\n",
    "    'GeminiAdvancedWeb-raw_correctness':\"GeminiAdvanced Web - no image\",\n",
    "    'GeminiAdvanced_GeminiAdvnacedVWebQ_correctness': \"GeminiAdvanced Web - LLM caption\",\n",
    "    'GeminiAdvnacedVWeb_Direct_correctness':  \"GeminiAdvanced Web- direct image\",\n",
    "    'GeminiAdvancedWeb_HumanCapQ_correctness':\"GeminiAdvanced Web - human hint\",}\n",
    "\n",
    "\n",
    "label_dic_GPT4API={\n",
    "    'gpt-4-0613_correctness':\"GPT4 API - no image\", \n",
    "    'gpt-4-0613-APIwLLMca_correctness': 'GPT4 API - LLM caption',\n",
    "    'gpt-4-vision-preview-APIDirect_correctness':'GPT4 API - direct image',\n",
    "    'gpt-4-0613-APIwHumanHint_correctness':'GPT4 API - human hint',}\n",
    "\n",
    "label_dic_ClaudeOpusAPI={\n",
    "    'claude-3-opus-20240229_correctness':\"Claude3Opus API - no image\", \n",
    "    'claude-3-opus-20240229-APIwLLMca_correctness':'Claude3Opus API - LLM caption',\n",
    "    'claude-3-opus-20240229-APIwDirect_correctness':'Claude3Opus API - direct image',\n",
    "    'claude-3-opus-20240229-APIwHumanHint_correctness':'Claude3Opus API - human hint'\n",
    "    }\n",
    "\n",
    "label_dic_ClaudeSonnetAPI={\n",
    "    'claude-3-sonnet-20240229_correctness':\"Claude3Sonnet API - no image\", \n",
    "    'claude-3-sonnet-20240229-APIwLLMca_correctness':'Claude3Sonnet API - LLM caption',\n",
    "    'claude-3-sonnet-20240229-APIwDirect_correctness':'Claude3Sonnet API - direct image',\n",
    "    'claude-3-sonnet-20240229-APIwHumanHint_correctness':'Claude3Sonnet API - human hint'\n",
    "    }\n",
    "\n",
    "list_of_label_dics = [label_dic_GPT4Web, label_dic_GPT4API, label_dic_ClaudeOpusWeb, label_dic_ClaudeOpusAPI, label_dic_ClaudeSonnetAPI, label_dic_GeminiAdvWeb ]\n",
    "\n",
    "# label_dic_GPT35_nodirect={\n",
    "#     'GPT3.5Web_correctness':\"GPT3.5 - (Web) no image\", \n",
    "#     'gpt-3.5-turbo-0125_correctness': \"GPT3.5 - (API) no image\", \n",
    "#     'GPT3.5Web_GPT4VWebCapQ_correctness': 'GPT3.5 - LLM caption',\n",
    "#     'GPT3.5VWeb_Direct_correctness':'GPT3.5 - direct image',\n",
    "#     'GPT3.5Web_HumanCapQ_correctness': 'GPT3.5 - human hint',}\n",
    "    \n",
    "# label_dic_remainingcol={\n",
    "#     #Claude\n",
    "#         'ClaudeSonnetWeb-raw_correctness': \"Claude3-Sonnet-Web\", \n",
    "#     # Gemini\n",
    "#            'GeminiWeb-raw_correctness':\"Gemini-Web\", \n",
    "#     #claudeAPI\n",
    "#     'ClaudeHiakuWeb-raw_correctness':\"Claude3Hiaku - (Web) no image\", \n",
    "#     'claude-3-sonnet-20240229_correctness':\"Claude3Hiaku - (API) no image\",\n",
    "#     'Claude3HaikuWeb_Claude3OpusWebCapQ_correctness':\"Claude3Hiaku - LLM caption\",\n",
    "#     'claude-3-haiku-20240307_correctness':\"Claude3-Sonnet-API\",}\n",
    "\n",
    "\n",
    "excel_file_path = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\__Milestone Datasets\\E2-2022-imageQ-CompleteCaptions_GenerationComplete_prepared_evaluated_Final_wAPIs.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def interactive_count_values(df, columns,replacement_dictionary=None):\n",
    "    # Dictionary to store counts of unique values for each column\n",
    "    counts_dict = {}\n",
    "    \n",
    "    # Dictionary to store replacements to avoid asking again\n",
    "    if replacement_dictionary is None:\n",
    "        replacement_dict = {}\n",
    "    else: \n",
    "        replacement_dict=replacement_dictionary\n",
    "    \n",
    "    for col in columns:\n",
    "        # Calculate counts of unique values for this column\n",
    "        counts = df[col].value_counts(normalize=True) * 100  # Convert to percentages\n",
    "        \n",
    "        # Display counts and ask for replacements\n",
    "        #print(f\"Counts for column '{col}':\")\n",
    "        #print(counts)\n",
    "        for value, percentage in counts.items():\n",
    "            # Check if a replacement is already stored\n",
    "            if value in replacement_dict:\n",
    "                replacement = replacement_dict[value]\n",
    "            else:\n",
    "                replacement = input(f\"Enter replacement for '{value}': \")\n",
    "                # Store replacement to avoid asking again\n",
    "                replacement_dict[value] = replacement\n",
    "                \n",
    "            # Replace value if needed\n",
    "            if replacement != \"\":\n",
    "                df[col] = df[col].replace({value: replacement})\n",
    "        \n",
    "        # Store counts for this column in the dictionary\n",
    "        counts_dict[col] = df[col].value_counts(normalize=True) * 100  # Update counts after replacements\n",
    "    \n",
    "    return counts_dict, replacement_dict\n",
    "\n",
    "def draw_line(ax, pos, length, text):\n",
    "    line_y = data.max() + 10  # slightly above the highest bar\n",
    "    ax.annotate(\n",
    "        text, \n",
    "        xy=(pos, line_y + 5), \n",
    "        xytext=(0, 0), textcoords=\"offset points\",\n",
    "        ha='center', va='bottom'\n",
    "    )\n",
    "    ax.plot([pos, pos + length], [line_y, line_y], color='black')\n",
    "\n",
    "# Load data from Excel file (replace 'excel_file_path' with your file path)\n",
    "excel_file_path = excel_file_path\n",
    "df = pd.read_excel(excel_file_path)\n",
    "df = df.apply(lambda x: re.sub('^Resolved: ', '', x) if isinstance(x, str) else x)\n",
    "\n",
    "# Identify the specific columns that we're intere sted in\n",
    "correctness_columns = [col for col in df.columns if col.endswith('_correctness')]\n",
    "\n",
    "replacement_dictionary=replacement_dictionary\n",
    "\n",
    "\n",
    "\n",
    "# Call the function to interactively count values and handle replacements\n",
    "model_correctness_counts, replacements = interactive_count_values(df, correctness_columns, replacement_dictionary=replacement_dictionary)\n",
    "\n",
    "# Print the final counted data for each column\n",
    "#print(\"\\nFinal counted data for each column:\")\n",
    "#for col, counts in model_correctness_counts.items():\n",
    "#    print(f\"Counts for column '{col}':\")\n",
    "#    print(counts)\n",
    "\n",
    "# Print the replacement dictionary\n",
    "print(\"\\nReplacement dictionary:\")\n",
    "print(replacements)\n",
    "\n",
    "\n",
    "# Transform the dictionary into a DataFrame\n",
    "df_plot = pd.DataFrame(model_correctness_counts).T.fillna(0)\n",
    "\n",
    "color_map = color_map\n",
    "# Reordering columns to match color map\n",
    "desired_columns = [key for key in color_map]\n",
    "valid_columns = [col for col in desired_columns if col in df_plot.columns]\n",
    "missing_columns = [col for col in desired_columns if col not in df_plot.columns]\n",
    "for col in missing_columns:\n",
    "    df_plot[col] = 0\n",
    "df_plot = df_plot[desired_columns]\n",
    "\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "#plt.savefig(r\"Submit\\Figures\\E1_Performance_horizental.jpg\", dpi=400)\n",
    "#plt.savefig(r\"Submit\\Figures\\E1_Performance_horizental.png\", dpi=400)\n",
    "#plt.savefig(r\"Submit\\Figures\\E1_Performance_horizental.pdf\", dpi=400)\n",
    "plt.show()\n",
    "df_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stat_labels=[]\n",
    "for label_dic in list_of_label_dics:\n",
    "    n=0\n",
    "    stat_dic={}\n",
    "    for key in label_dic.keys():\n",
    "        if n !=0:\n",
    "            stat_dic[key_refrence].append(key)\n",
    "        if n==0:\n",
    "            key_refrence = key\n",
    "            stat_dic[key_refrence] = []\n",
    "        n+=1\n",
    "    list_of_stat_labels.append(stat_dic)\n",
    "\n",
    "list_of_stat_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first calculate the stat and then go to plot. You should use this df for future use after plotting\n",
    "list_of_stat_labels=[]\n",
    "for label_dic in list_of_label_dics:\n",
    "    n=0\n",
    "    stat_dic={}\n",
    "    for key in label_dic.keys():\n",
    "        if n !=0:\n",
    "            stat_dic[key_refrence].append(key)\n",
    "        if n==0:\n",
    "            key_refrence = key\n",
    "            stat_dic[key_refrence] = []\n",
    "        n+=1\n",
    "    list_of_stat_labels.append(stat_dic)\n",
    "\n",
    "df_counts = df_plot.copy()\n",
    "df_counts['Correct'] = (df_counts['Correct'] * 3).round().astype(int)\n",
    "df_counts['Incorrect'] = (df_counts['Incorrect'] * 3).round().astype(int)\n",
    "\n",
    "# Function to perform chi-square test\n",
    "def chi_square_test(df_counts, reference, comparison):\n",
    "    observed = [\n",
    "        [df_counts.at[reference, 'Correct'], df_counts.at[reference, 'Incorrect']],\n",
    "        [df_counts.at[comparison, 'Correct'], df_counts.at[comparison, 'Incorrect']]\n",
    "    ]\n",
    "    chi2, p, dof, ex = chi2_contingency(observed)\n",
    "    \n",
    "    return p\n",
    "\n",
    "def add_stat_annotation(p_val):\n",
    "    if p_val < 0.0001:\n",
    "        annotation = '****'\n",
    "    elif p_val < 0.001:\n",
    "        annotation = '***'\n",
    "    elif p_val < 0.01:\n",
    "        annotation = '**'\n",
    "    elif p_val < 0.05:\n",
    "        annotation = '*'\n",
    "    elif p_val >=0.05:\n",
    "        annotation = 'ns'\n",
    "    else:\n",
    "        annotation = ' '\n",
    "    return annotation\n",
    "        \n",
    "        \n",
    "df_counts['p_value'] = None\n",
    "df_counts['p_value_annotation'] = None\n",
    "for comparison_dict in list_of_stat_labels:\n",
    "    for reference, comparisons in comparison_dict.items():\n",
    "        for comparison in comparisons:\n",
    "            p_value = chi_square_test(df_counts, reference, comparison)\n",
    "            df_counts.at[comparison, 'p_value'] = p_value\n",
    "            df_counts.at[comparison, 'p_value_annotation']= add_stat_annotation(p_value)\n",
    "            df_counts.at[reference, 'p_value_annotation'] =r'(ref)'\n",
    "for label_dic in list_of_label_dics:\n",
    "    for old_name, new_name in label_dic.items():\n",
    "        df_counts.rename(index={old_name: new_name}, inplace=True)\n",
    "df_counts      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "df_plot2=df_plot.copy()\n",
    "\n",
    "# Rename and reorder the dataframe based on list_of_label_dics\n",
    "ordered_indices = []\n",
    "for label_dic in list_of_label_dics:\n",
    "    for old_name, new_name in label_dic.items():\n",
    "        df_plot2.rename(index={old_name: new_name}, inplace=True)\n",
    "        ordered_indices.append(new_name)\n",
    "\n",
    "# Add NaN rows to create empty bars\n",
    "extended_indices = []\n",
    "for i, dic in enumerate(list_of_label_dics):\n",
    "    for key in dic.values():\n",
    "        extended_indices.append(key)\n",
    "    if i < len(list_of_label_dics) - 1:  # Add NaN between clusters except after the last one\n",
    "        extended_indices.append(f'empty_{i}')\n",
    "    \n",
    "\n",
    "df_extended = df_plot2.reindex(extended_indices).fillna(0)\n",
    "\n",
    "# Reversing the order of indices for plotting\n",
    "df_extended = df_extended.iloc[::-1]\n",
    "\n",
    "# Plotting the stacked bar chart\n",
    "plt.figure(figsize=(8, 12))\n",
    "\n",
    "bottom = None\n",
    "for key in color_map:\n",
    "    values = df_extended[key]\n",
    "    plt.barh(df_extended.index, values, left=bottom, color=color_map[key], label=key)\n",
    "    if key == 'Correct':\n",
    "        for i, value in enumerate(values):\n",
    "            if value > 0:\n",
    "                plt.text((value if bottom is None else bottom[i] + value) - (12 if key == 'Correct' else -5), i, \n",
    "                         f\"{value:.1f}%\", va='center', ha='left',\n",
    "                         fontdict={'fontname': 'Arial', 'fontsize': 8}, color='black',\n",
    "                         bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.3', alpha=0.5))\n",
    "\n",
    "\n",
    "    if bottom is None:\n",
    "        bottom = values\n",
    "    else:\n",
    "        bottom += values\n",
    "\n",
    "for model in df_counts.index.to_list():\n",
    "    if model in df_extended.index:\n",
    "        plt.text(107, model, df_counts.at[model, 'p_value_annotation'])\n",
    "\n",
    "\n",
    "\n",
    "plt.grid(True, axis='x', color='white', linestyle='-', linewidth=0.5) \n",
    "plt.gca().set_facecolor('lightgray') \n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.legend(loc='lower right',)\n",
    "\n",
    "# Adding labels, title and legend\n",
    "plt.xlabel('Percentage')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Hide y-axis labels for specified empty indices\n",
    "ax = plt.gca()\n",
    "y_labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "y_labels = [label if 'empty' not in label else '' for label in y_labels]\n",
    "ax.set_yticklabels(y_labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"Submit\\Figures\\E2_performance_horizental.jpg\", dpi=400)\n",
    "plt.savefig(r\"Submit\\Figures\\E2_performance_horizental.png\", dpi=400)\n",
    "plt.savefig(r\"Submit\\Figures\\E2_performance_horizental.pdf\", dpi=400)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 15})\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample dataframe creation - replace this with your actual dataframe\n",
    "E3_excel_file_path=r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\__Milestone Datasets\\E3b_GPT.xlsx\"\n",
    "df = pd.read_excel(E3_excel_file_path)\n",
    "\n",
    "\n",
    "# Find columns ending with '_correctness'\n",
    "correctness_columns = [col for col in df.columns if col.endswith('_correctness')]\n",
    "\n",
    "# Calculate accuracy for each correctness column\n",
    "accuracies = {}\n",
    "for col in correctness_columns:\n",
    "    accuracy = (df[col] == 'correct').sum() / len(df)\n",
    "    accuracies[col] = accuracy\n",
    "\n",
    "# User input for labels and years\n",
    "labels_years = { \n",
    "                'babbage-002_correctness': ('GPT3: babbage-002', '2021-09','#AF8260'), \n",
    "                'davinci-002_correctness': ('GPT3: davinci-002', '2021-09', '#AF8260'), \n",
    "                'gpt-3.5-turbo-instruct_correctness': ('GPT3.5-Instruct', '2021-09', 'orange'), \n",
    "                'gpt-3.5-turbo-0125_correctness': ('GPT3.5-0125', '2023-12','orange'), \n",
    "                'gpt-4-0613_correctness': ('GPT4-0613', '2021-09', 'blue'), \n",
    "                'gpt-3.5-turbo-0613_correctness': ('GPT3.5-0613', '2021-09','orange'), \n",
    "                'gpt-3.5-turbo-1106_correctness': ('GPT3.5-1106', '2023-04','orange'), \n",
    "                'gpt-4-1106-preview_correctness': ('GPT4-1106', '2023-04','blue'), \n",
    "                'gpt-4-0125-preview_correctness': ('GPT4-0125', '2023-12','blue'), \n",
    "                'gpt-4o-2024-05-13_correctness':('GPT4o-20240513', '2023-09','#640D6B'),\n",
    "                'gpt-4o-mini-2024-07-18_correctness':('GPT4omini-20240718', '2023-09','#640D6B')}\n",
    "\n",
    "for col in accuracies:\n",
    "\n",
    "    if col not in labels_years:\n",
    "        label = input(f\"Enter a label for {col}: \")\n",
    "        year = input(f\"Enter the year for {col} (extracted from column name): \")\n",
    "        color=year = input(f\"Enter the color for {col}: \")\n",
    "        labels_years[col] = (label, year,color)\n",
    "    print(f\"Column: {col}, Accuracy: {accuracies[col]*100:.2f}% -> {labels_years[col]} \")\n",
    "\n",
    "print(labels_years)\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Color\n",
    "plt.grid(True, axis='y', color='white', linestyle='-', linewidth=0.5)  # Change grid color to white\n",
    "plt.gca().set_facecolor('lightgray')  # Set background color to gray\n",
    "# Remove top and right spines\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "for col, accuracy in accuracies.items():\n",
    "    label, year, color = labels_years[col]\n",
    "    plt.scatter(year, accuracy * 100, label=f\"{label} ({year})\", s=100, color= color)  # Size of point set to 100 for visibility\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Model Training Data Cutoff Date')\n",
    "plt.ylabel('Percentage of Correct Answers')\n",
    "#plt.title('Accuracy of GPT Models Over Years')\n",
    "plt.legend(title=\"Model (Training Data Date)\")\n",
    "\n",
    "save_path=r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\Submit\\Figures\\newE3b\"\n",
    "plt.savefig(f'{save_path}.pdf', dpi=500, bbox_inches='tight')  # Save as PDF with higher resolution\n",
    "plt.savefig(f'{save_path}.png', dpi=500, bbox_inches='tight')  # Save as PNG with higher resolution\n",
    "plt.savefig(f'{save_path}.jpg', dpi=500, bbox_inches='tight')  # Save as JPG with higher resolution\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Validation of Semi-Automated Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT3.5 Evaluation Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extraction_column='Extracted Answer Clean for confusion matrix'\n",
    "Human_Validation_column='Huamn Extracted Answer'\n",
    "\n",
    "validation_excel_path=r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\__Milestone Datasets\\GPT3.5Extractor Validation.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel(validation_excel_path)\n",
    "\n",
    "# Drop missing values to ensure clean data for analysis\n",
    "df = df.dropna(subset=[Human_Validation_column, Extraction_column])\n",
    "\n",
    "# Generate a cross-tabulation of counts between the two columns\n",
    "confusion_data = pd.crosstab(df[Human_Validation_column], df[Extraction_column])\n",
    "\n",
    "# Reorder the rows and columns based on specified sort orders\n",
    "confusion_data = confusion_data.reindex(index=true_labels_sort_order, columns=extracted_labels_sort_order)\n",
    "\n",
    "# Any missing rows or columns after reindexing will be filled with zeros\n",
    "confusion_data = confusion_data.fillna(0).astype(int)\n",
    "\n",
    "# Plotting the confusion data as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.heatmap(confusion_data, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('GPT Extracted Option from LLM Textual Response')\n",
    "plt.ylabel('Blinded Human Validation')\n",
    "\n",
    "# Move x-axis to top and rotate specific labels\n",
    "ax.xaxis.tick_top()  # Move x-axis to top\n",
    "ax.xaxis.set_label_position('top')  # Move the x-axis label to top\n",
    "\n",
    "# Rotate all x and y tick labels for clarity\n",
    "plt.xticks(rotation=45, ha='left')  # Horizontal alignment can be adjusted\n",
    "plt.yticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"Submit\\Figures\\GPT_Extract_Validation.jpg\", dpi=400)\n",
    "plt.savefig(r\"Submit\\Figures\\GPT_Extract_Validation.png\", dpi=400)\n",
    "plt.savefig(r\"Submit\\Figures\\GPT_Extract_Validation.pdf\", dpi=400)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel(validation_excel_path)\n",
    "\n",
    "# Drop missing values to ensure clean data for analysis\n",
    "df = df.dropna(subset=[Human_Validation_column, Extraction_column])\n",
    "\n",
    "# Generate a cross-tabulation of counts between the two columns\n",
    "confusion_data = pd.crosstab(df[Human_Validation_column], df[Extraction_column])\n",
    "\n",
    "# Reorder the rows and columns based on specified sort orders\n",
    "confusion_data = confusion_data.reindex(index=true_labels_sort_order, columns=extracted_labels_sort_order)\n",
    "\n",
    "# Any missing rows or columns after reindexing will be filled with zeros\n",
    "confusion_data = confusion_data.fillna(0).astype(int)\n",
    "\n",
    "# Plotting the confusion data as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(confusion_data, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('GPT-3.5 Extracted Option from LLM Textual Response',  fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.ylabel('Blinded Human Validation',  fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "\n",
    "# Move x-axis to top and rotate specific labels\n",
    "ax.xaxis.tick_top()  # Move x-axis to top\n",
    "ax.xaxis.set_label_position('top')  # Move the x-axis label to top\n",
    "\n",
    "# Rotate all x and y tick labels for clarity\n",
    "plt.xticks(rotation=20, ha='left')  # Horizontal alignment can be adjusted\n",
    "plt.yticks(rotation=20)\n",
    "\n",
    "# Define labels to highlight\n",
    "highlight_labels = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "# Draw rectangles around specified labels on both axes\n",
    "for label in ax.get_xticklabels():\n",
    "    if label.get_text() in highlight_labels:\n",
    "        # Find the position of the label in the list\n",
    "        index = extracted_labels_sort_order.index(label.get_text())\n",
    "        # Draw a rectangle around the column\n",
    "        ax.add_patch(Rectangle((index, 0), 1, 5, fill=False, edgecolor='#ff0080', lw=1, alpha=0.3))\n",
    "\n",
    "\n",
    "for label in ax.get_yticklabels():\n",
    "    if label.get_text() in highlight_labels:\n",
    "        # Find the position of the label in the list\n",
    "        index = true_labels_sort_order.index(label.get_text())\n",
    "        # Draw a rectangle around the row\n",
    "        ax.add_patch(Rectangle((0, index), 5, 1, fill=False, edgecolor='#ff0080', lw=1, alpha=0.3))\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Manually Evaluated answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E1_excel_file_path= r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\__Milestone Datasets\\E1_Bestprompt-2022_all_Final_E2_manualevaluatedcompleted_Final_E2_Final.xlsx\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def count_and_percent_non_standard_answers(excel_file_path, specific_columns=None):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_excel(excel_file_path)\n",
    "    \n",
    "    # Determine which columns to analyze\n",
    "    if specific_columns is not None:\n",
    "        answer_columns = [col for col in specific_columns if col in df.columns and col.endswith('_answer')]\n",
    "    else:\n",
    "        answer_columns = [col for col in df.columns if col.endswith('_answer')]\n",
    "    \n",
    "    # Initialize a dictionary to store the counts and percentages\n",
    "    results = {}\n",
    "    overall_total_count = 0\n",
    "    overall_non_standard_count = 0\n",
    "    \n",
    "    # Valid answers set\n",
    "    valid_answers = {'A', 'B', 'C', 'D', 'E'}\n",
    "    \n",
    "    # Calculate counts and percentages for each column\n",
    "    for col in answer_columns:\n",
    "        total_count = 300  # Count of non-null answers\n",
    "        non_standard_count = df[col][~df[col].isin(valid_answers)].count()\n",
    "        percentage = (non_standard_count / total_count) * 100 if total_count > 0 else 0\n",
    "        \n",
    "        # Update overall totals\n",
    "        overall_total_count += total_count\n",
    "        overall_non_standard_count += non_standard_count\n",
    "        \n",
    "        results[col] = {'Total Count': total_count, 'Non-Standard Count': non_standard_count, 'Percentage': percentage}\n",
    "    \n",
    "    # Calculate overall percentages\n",
    "    overall_percentage = (overall_non_standard_count / overall_total_count) * 100 if overall_total_count > 0 else 0\n",
    "    \n",
    "    # Store overall results\n",
    "    results['Overall'] = {'Total Count': overall_total_count, 'Non-Standard Count': overall_non_standard_count, 'Percentage': overall_percentage}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# excel_file_path = 'path_to_your_excel_file.xlsx'\n",
    "# specific_columns = ['math_answer', 'science_answer']  # Optional: specify columns\n",
    "# print(count_and_percent_non_standard_answers(excel_file_path, specific_columns))\n",
    "\n",
    "\n",
    "results = count_and_percent_non_standard_answers(E1_excel_file_path)\n",
    "print(results)\n",
    "\n",
    "# Example usage:\n",
    "# excel_file_path = 'path_to_your_excel_file.xlsx'\n",
    "# print(count_non_standard_answers(excel_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel(E1_excel_file_path)\n",
    "# answer_columns = [col for col in df.columns if col.endswith('_answer')]\n",
    "\n",
    "\n",
    "desired_columns_list = ['gpt-3.5-turbo-0125_answer',\n",
    "    'gpt-4-0613_answer',\n",
    "    'gpt-4o-2024-05-13_answer',\n",
    "    'gpt-4o-mini-2024-07-18_answer',\n",
    "\n",
    "    'claude-3-5-sonnet-20240620_answer',\n",
    "    'claude-3-opus-20240229_answer',\n",
    "    'claude-3-haiku-20240307_answer',\n",
    "    'claude-3-sonnet-20240229_answer',]\n",
    "\n",
    "results = count_and_percent_non_standard_answers(E1_excel_file_path, desired_columns_list)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Image Resoulution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def get_image_stats(directory):\n",
    "    dimensions = []\n",
    "    resolutions = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with Image.open(filepath) as img:\n",
    "                width, height = img.size\n",
    "                dpi = img.info.get('dpi', (0, 0))\n",
    "                \n",
    "                dimensions.append((width, height))\n",
    "                resolutions.append(dpi)\n",
    "\n",
    "    if dimensions:\n",
    "        widths, heights = zip(*dimensions)\n",
    "        avg_width = np.mean(widths)\n",
    "        avg_height = np.mean(heights)\n",
    "        std_width = np.std(widths)\n",
    "        std_height = np.std(heights)\n",
    "    else:\n",
    "        avg_width = avg_height = std_width = std_height = 0\n",
    "\n",
    "    if resolutions:\n",
    "        x_dpi, y_dpi = zip(*resolutions)\n",
    "        avg_x_dpi = np.mean(x_dpi)\n",
    "        avg_y_dpi = np.mean(y_dpi)\n",
    "        std_x_dpi = np.std(x_dpi)\n",
    "        std_y_dpi = np.std(y_dpi)\n",
    "    else:\n",
    "        avg_x_dpi = avg_y_dpi = std_x_dpi = std_y_dpi = 0\n",
    "\n",
    "    return {\n",
    "        'average_dimensions': (avg_width, avg_height),\n",
    "        'std_dimensions': (std_width, std_height),\n",
    "        'average_resolution': (avg_x_dpi, avg_y_dpi),\n",
    "        'std_resolution': (std_x_dpi, std_y_dpi)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "directory_path = r\"C:\\Users\\LEGION\\Documents\\GIT\\LLM_answer_GIBoard\\DO_NOT_PUBLISH\\ACG self asses\\2022\"\n",
    "stats = get_image_stats(directory_path)\n",
    "print(stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "native",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
